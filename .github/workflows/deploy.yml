name: Deploy Subscriber Migration Portal

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Check AWS Account ID
        run: |
          echo "Checking AWS identity..."
          aws sts get-caller-identity
      
      # ---------------------
      # Step 1: Build & Deploy Backend
      # ---------------------
      - name: Setup Python
        uses: actions/setup-python@v5 
        with:
          python-version: '3.11' 

      - name: Install and Package All Backends
        run: |
          echo "Current directory: $(pwd)"
          echo "Listing files in backend: $(ls -la backend)"
          echo "--- Packaging Main Flask API ---"
          cd backend
          pip install --no-cache-dir -r requirements.txt -t .
          # Copy shared files needed by the migration processor into its directory
          if [ -f "legacy_db.py" ]; then
            cp legacy_db.py migration_processor/
          else
            echo "::warning:: legacy_db.py not found in backend/ - Processor might fail if it needs it."
          fi
           if [ -f "bulk_ops.py" ]; then # Only needed if processor still uses MOCK
            cp bulk_ops.py migration_processor/
          else
            echo "::warning:: bulk_ops.py not found in backend/ - Processor might fail if it needs MOCK data."
          fi
          # Create zip file in the root directory, excluding specific files/dirs
          zip -r ../backend.zip . -x "*.pyc" "__pycache__/*" "*.git*" "migration_processor/*"
          cd .. # Return to project root
          echo "Current directory after main API packaging: $(pwd)"

          echo "--- Packaging Migration Processor ---"
          if [ -d "backend/migration_processor" ]; then
            cd backend/migration_processor
            echo "Current directory for processor packaging: $(pwd)"
            echo "Listing files in migration_processor: $(ls -la .)"
            # Check if requirements.txt exists BEFORE running pip
            if [ ! -f "requirements.txt" ]; then
               echo "::error:: File 'backend/migration_processor/requirements.txt' not found inside directory!"
               exit 1
            fi
            # Install dependencies (will do nothing if requirements.txt is empty)
            echo "Running pip install in $(pwd)..."
            pip install --no-cache-dir -r requirements.txt -t .
            # Create zip file in the project root directory
            zip -r ../../processor.zip . -x "*.pyc" "__pycache__/*"
            cd ../.. # Return to project root
            echo "Current directory after processor packaging: $(pwd)"
          else
            echo "::error:: The 'backend/migration_processor' directory was not found."
            exit 1
          fi

      - name: Deploy CloudFormation Stack
        run: |
          cd aws
          aws cloudformation deploy \
            --template-file cloudformation.yaml \
            --stack-name subscriber-migration-stack-v3 \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --parameter-overrides \
              DomainName=${{ secrets.DOMAIN_NAME }} \
              LegacyDBPassword=${{ secrets.LEGACY_DB_PASSWORD }}

      - name: Get CloudFormation Outputs
        id: cfn_outputs
        run: |
          STACK_NAME="subscriber-migration-stack-v3"
          FUNCTION_NAME=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='BackendLambdaName'].OutputValue" --output text)
          SUBSCRIBER_TABLE=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='SubscriberTableName'].OutputValue" --output text)
          AUDIT_TABLE=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='AuditLogTableName'].OutputValue" --output text)
          API_ID=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='BackendApiId'].OutputValue" --output text)
          API_URL=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='BackendApiUrl'].OutputValue" --output text)
          FRONTEND_URL=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='FrontendURL'].OutputValue" --output text)
          MIG_JOBS_TABLE=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='MigrationJobsTableName'].OutputValue" --output text)
          MIG_UPLOAD_BUCKET=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='MigrationUploadBucketName'].OutputValue" --output text)
          MIG_PROCESSOR_NAME=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='MigrationProcessorFunctionName'].OutputValue" --output text)
          LEGACY_DB_SECRET_ARN=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='LegacyDBSecretArn'].OutputValue" --output text)
          LEGACY_DB_HOST=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='LegacyDBEndpoint'].OutputValue" --output text)

          echo "function_name=$FUNCTION_NAME" >> $GITHUB_OUTPUT
          echo "subscriber_table=$SUBSCRIBER_TABLE" >> $GITHUB_OUTPUT
          echo "audit_table=$AUDIT_TABLE" >> $GITHUB_OUTPUT
          echo "api_id=$API_ID" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT
          echo "mig_jobs_table=$MIG_JOBS_TABLE" >> $GITHUB_OUTPUT
          echo "mig_upload_bucket=$MIG_UPLOAD_BUCKET" >> $GITHUB_OUTPUT
          echo "mig_processor_name=$MIG_PROCESSOR_NAME" >> $GITHUB_OUTPUT
          echo "legacy_db_secret_arn=$LEGACY_DB_SECRET_ARN" >> $GITHUB_OUTPUT
          echo "legacy_db_host=$LEGACY_DB_HOST" >> $GITHUB_OUTPUT

          echo "::notice:: API URL: $API_URL"
          echo "::notice:: Frontend URL: $FRONTEND_URL"

      - name: Update backend Lambda code
        run: |
          aws lambda update-function-code \
            --function-name ${{ steps.cfn_outputs.outputs.function_name }} \
            --zip-file fileb://backend.zip

      - name: Update MAIN Lambda Environment Variables
        run: |
          aws lambda update-function-configuration \
            --function-name ${{ steps.cfn_outputs.outputs.function_name }} \
            --environment "Variables={ \
              FRONTEND_DOMAIN_URL=${{ steps.cfn_outputs.outputs.frontend_url }}, \
              SUBSCRIBER_TABLE_NAME=${{ steps.cfn_outputs.outputs.subscriber_table }}, \
              AUDIT_LOG_TABLE_NAME=${{ steps.cfn_outputs.outputs.audit_table }}, \
              MIGRATION_JOBS_TABLE_NAME=${{ steps.cfn_outputs.outputs.mig_jobs_table }}, \
              MIGRATION_UPLOAD_BUCKET_NAME=${{ steps.cfn_outputs.outputs.mig_upload_bucket }}, \
              LEGACY_DB_SECRET_ARN=${{ steps.cfn_outputs.outputs.legacy_db_secret_arn }}, \
              LEGACY_DB_HOST=${{ steps.cfn_outputs.outputs.legacy_db_host }} \
            }"

      - name: Upload MIGRATION Processor code to S3
        env:
          FRONTEND_URL: ${{ steps.cfn_outputs.outputs.frontend_url }}
        run: |
          BUCKET_NAME=$(echo $FRONTEND_URL | sed 's/http:\/\///' | sed 's/.s3-website.*//')
          aws s3 cp processor.zip s3://$BUCKET_NAME/processor.zip

      - name: Update MIGRATION Processor Lambda code
        env:
          FRONTEND_URL: ${{ steps.cfn_outputs.outputs.frontend_url }}
          MIG_PROCESSOR_NAME: ${{ steps.cfn_outputs.outputs.mig_processor_name }}
        run: |
          BUCKET_NAME=$(echo $FRONTEND_URL | sed 's/http:\/\///' | sed 's/.s3-website.*//')
          aws lambda update-function-code \
            --function-name $MIG_PROCESSOR_NAME \
            --s3-bucket $BUCKET_NAME \
            --s3-key "processor.zip"
      
      - name: Update MIGRATION Lambda Environment Variables
        run: |
           aws lambda update-function-configuration \
            --function-name ${{ steps.cfn_outputs.outputs.mig_processor_name }} \
            --environment "Variables={ \
              SUBSCRIBER_TABLE_NAME=${{ steps.cfn_outputs.outputs.subscriber_table }}, \
              AUDIT_LOG_TABLE_NAME=${{ steps.cfn_outputs.outputs.audit_table }}, \
              MIGRATION_JOBS_TABLE_NAME=${{ steps.cfn_outputs.outputs.mig_jobs_table }}, \
              MIGRATION_UPLOAD_BUCKET_NAME=${{ steps.cfn_outputs.outputs.mig_upload_bucket }}, \
              REPORT_BUCKET_NAME=${{ steps.cfn_outputs.outputs.mig_upload_bucket }}, \
              LEGACY_DB_SECRET_ARN=${{ steps.cfn_outputs.outputs.legacy_db_secret_arn }}, \
              LEGACY_DB_HOST=${{ steps.cfn_outputs.outputs.legacy_db_host }} \
            }"

      - name: Wait for Lambda function updates to complete
        run: |
          aws lambda wait function-updated --function-name ${{ steps.cfn_outputs.outputs.function_name }}
          aws lambda wait function-updated --function-name ${{ steps.cfn_outputs.outputs.mig_processor_name }}

      - name: Deploy API Gateway Stage
        env:
          API_ID: ${{ steps.cfn_outputs.outputs.api_id }}
        run: |
          echo "Starting deployment for API Gateway: $API_ID"
          DEPLOYMENT_ID=$(aws apigateway create-deployment \
            --rest-api-id $API_ID \
            --description "CI/CD code update $(date +%s)" \
            --query 'id' \
            --output text --region us-east-1)
          
          if [ -z "$DEPLOYMENT_ID" ]; then
            echo "Failed to create deployment"
            exit 1
          fi
          
          aws apigateway update-stage \
            --rest-api-id $API_ID \
            --stage-name prod \
            --patch-operations op='replace',path='/deploymentId',value=$DEPLOYMENT_ID
          
          echo "API Gateway stage 'prod' updated"

      # ---------------------
      # Step 2: Build & Deploy Frontend
      # ---------------------
      - name: Install frontend dependencies
        run: |
          cd frontend
          CI=false npm install

      - name: Build frontend
        run: |
          cd frontend
          CI=false npm run build

      - name: Deploy frontend to S3
        run: |
          aws s3 sync frontend/build/ s3://subscriber-portal-${{ secrets.AWS_ACCOUNT_ID }}-us-east-1 --delete
