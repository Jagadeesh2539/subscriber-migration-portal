name: Deploy CloudFormation Stack with Ultra-Robust Auto-Management

on:
  push:
    branches: [ main ]
    paths:
      - 'aws/**'
      - '.github/workflows/**'
      - 'backend/**'
      - 'migration-processor/**'
      - 'frontend/**'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      delete_stack_only:
        description: 'Delete stack only (true/false)'
        required: false
        default: 'false'
      force_recreate:
        description: 'Force recreate stack (true/false)'
        required: false
        default: 'false'
      cleanup_all_resources:
        description: 'Cleanup all orphaned resources (true/false)'
        required: false
        default: 'false'
      skip_validation:
        description: 'Skip template validation (true/false)'
        required: false
        default: 'false'
      s3_prefix:
        description: 'S3 key prefix'
        required: false
        default: 'uploads/'
      s3_suffix:
        description: 'S3 key suffix'
        required: false
        default: '.csv'

env:
  AWS_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-main
  TEMPLATE_FILE: aws/cloudformation.yaml
  MAX_DEPLOYMENT_ATTEMPTS: 5
  DEPLOYMENT_TIMEOUT: 2400
  RETRY_DELAY: 120

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install and configure AWS CLI
      run: |
        set -euo pipefail
        
        # Install AWS CLI v2
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip -q awscliv2.zip
        sudo ./aws/install --update
        
        # Install additional tools
        sudo apt-get update
        sudo apt-get install -y jq mysql-client zip python3 python3-pip
        curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
        sudo apt-get install -y nodejs
        
        # Verify installation
        aws --version
        jq --version
        node --version
        npm --version
        python3 --version
        python3 -m pip --version
        
        # Configure AWS CLI for better performance
        aws configure set max_concurrent_requests 20
        aws configure set max_queue_size 10000
        aws configure set region ${{ env.AWS_REGION }}
        aws configure set output json

    - name: Verify AWS credentials and permissions
      run: |
        set -euo pipefail
        
        echo "üîê Testing AWS credentials..."
        
        # Get caller identity
        CALLER_IDENTITY=$(aws sts get-caller-identity)
        echo "‚úÖ Caller Identity: $CALLER_IDENTITY"
        
        # Test CloudFormation permissions
        echo "üîç Testing CloudFormation permissions..."
        aws cloudformation list-stacks --query 'StackSummaries[0]' --output table || echo "‚ö†Ô∏è  Limited CloudFormation permissions"
        
        # Test S3 permissions
        echo "ü™£ Testing S3 permissions..."
        aws s3 ls || echo "‚ö†Ô∏è  Limited S3 permissions"

    - name: Create global functions file
      run: |
        # Create a global functions file that can be sourced by all steps
        cat > /tmp/stack_functions.sh << 'FUNCTIONS_EOF'
        #!/bin/bash
        
        # Enhanced stack existence check (FIXED: added || false to prevent step termination)
        stack_exists() {
          aws cloudformation describe-stacks --stack-name "$1" --region "$2" &>/dev/null || false
        }
        
        # Enhanced stack status retrieval
        get_stack_status() {
          if aws cloudformation describe-stacks --stack-name "$1" --region "$2" &>/dev/null; then
            aws cloudformation describe-stacks --stack-name "$1" --region "$2" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "UNKNOWN"
          else
            echo "STACK_NOT_EXISTS"
          fi
        }
        
        # Enhanced S3 bucket deletion function
        delete_s3_bucket_enhanced() {
          local bucket_name="$1"
          local max_retries=3
          local retry=1
          
          # Check if bucket exists
          if ! aws s3api head-bucket --bucket "$bucket_name" 2>/dev/null; then
            echo "‚ÑπÔ∏è  Bucket does not exist: $bucket_name"
            return 0
          fi
          
          echo "üóëÔ∏è  Processing bucket: $bucket_name"
          
          while [[ $retry -le $max_retries ]]; do
            echo "üîÑ Attempt $retry of $max_retries for bucket: $bucket_name"
            
            # Get bucket region
            BUCKET_REGION=$(aws s3api get-bucket-location --bucket "$bucket_name" --query 'LocationConstraint' --output text 2>/dev/null || echo "us-east-1")
            [[ "$BUCKET_REGION" == "None" || "$BUCKET_REGION" == "null" ]] && BUCKET_REGION="us-east-1"
            
            # Check bucket versioning
            VERSIONING=$(aws s3api get-bucket-versioning --bucket "$bucket_name" --region "$BUCKET_REGION" --query 'Status' --output text 2>/dev/null || echo "None")
            
            # Handle versioned buckets
            if [[ "$VERSIONING" == "Enabled" ]]; then
              echo "üì¶ Removing all versions from versioned bucket..."
              
              # Delete all object versions
              aws s3api list-object-versions --bucket "$bucket_name" --region "$BUCKET_REGION" \
                --query 'Versions[].{Key:Key,VersionId:VersionId}' --output text 2>/dev/null | \
              while read -r key version_id; do
                if [[ -n "$key" && -n "$version_id" && "$key" != "None" && "$version_id" != "None" ]]; then
                  aws s3api delete-object --bucket "$bucket_name" --key "$key" --version-id "$version_id" --region "$BUCKET_REGION" 2>/dev/null || true
                fi
              done
              
              # Delete all delete markers
              aws s3api list-object-versions --bucket "$bucket_name" --region "$BUCKET_REGION" \
                --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' --output text 2>/dev/null | \
              while read -r key version_id; do
                if [[ -n "$key" && -n "$version_id" && "$key" != "None" && "$version_id" != "None" ]]; then
                  aws s3api delete-object --bucket "$bucket_name" --key "$key" --version-id "$version_id" --region "$BUCKET_REGION" 2>/dev/null || true
                fi
              done
            fi
            
            # Remove all objects
            echo "üßπ Removing all objects..."
            aws s3 rm "s3://$bucket_name" --recursive --region "$BUCKET_REGION" 2>/dev/null || true
            
            # Remove bucket configurations
            aws s3api delete-bucket-policy --bucket "$bucket_name" --region "$BUCKET_REGION" 2>/dev/null || true
            aws s3api put-bucket-notification-configuration --bucket "$bucket_name" --notification-configuration "{}" --region "$BUCKET_REGION" 2>/dev/null || true
            
            # Delete the bucket
            if aws s3api delete-bucket --bucket "$bucket_name" --region "$BUCKET_REGION" 2>/dev/null; then
              echo "‚úÖ Bucket deleted successfully: $bucket_name"
              return 0
            else
              echo "‚ö†Ô∏è  Failed to delete bucket on attempt $retry: $bucket_name"
              retry=$((retry + 1))
              [[ $retry -le $max_retries ]] && sleep 10
            fi
          done
          
          echo "‚ùå Failed to delete bucket after $max_retries attempts: $bucket_name"
          return 1
        }
        
        # Enhanced stack deletion with retry logic
        delete_failed_stack_robust() {
          local stack_name="$1"
          local region="$2"
          local max_retries=3
          local retry=1
          
          echo "üóëÔ∏è  Initiating robust stack deletion: $stack_name"
          
          # First check if stack actually exists
          if ! stack_exists "$stack_name" "$region"; then
            echo "‚ÑπÔ∏è  Stack does not exist: $stack_name"
            return 0
          fi
          
          while [[ $retry -le $max_retries ]]; do
            echo "üîÑ Deletion attempt $retry of $max_retries"
            
            # Get current status
            local current_status=$(get_stack_status "$stack_name" "$region")
            echo "üìä Current status: $current_status"
            
            # Handle different statuses
            case "$current_status" in
              DELETE_COMPLETE|STACK_NOT_EXISTS)
                echo "‚úÖ Stack deletion completed"
                return 0
                ;;
              DELETE_IN_PROGRESS)
                echo "‚è≥ Stack deletion already in progress, waiting..."
                ;;
              *)
                echo "üóëÔ∏è  Initiating stack deletion..."
                aws cloudformation delete-stack --stack-name "$stack_name" --region "$region" 2>/dev/null || true
                ;;
            esac
            
            # Wait for deletion with timeout
            echo "‚è≥ Waiting for stack deletion (timeout: 30 minutes)..."
            if timeout 1800 aws cloudformation wait stack-delete-complete --stack-name "$stack_name" --region "$region" 2>/dev/null; then
              echo "‚úÖ Stack deleted successfully: $stack_name"
              return 0
            else
              echo "‚ö†Ô∏è  Stack deletion timeout or failed on attempt $retry"
              retry=$((retry + 1))
              [[ $retry -le $max_retries ]] && sleep 60
            fi
          done
          
          echo "‚ùå Failed to delete stack after $max_retries attempts: $stack_name"
          return 1
        }
        
        # Wait for Lambda function to be ready for next operation
        wait_for_lambda_ready() {
          local function_name="$1"
          local operation_type="$2"
          local max_wait_time=300
          local wait_interval=10
          local elapsed_time=0
          
          echo "‚è≥ Waiting for Lambda function '$function_name' to be ready for $operation_type..."
          
          while [[ $elapsed_time -lt $max_wait_time ]]; do
            # Get current function state
            FUNCTION_STATE=$(aws lambda get-function --function-name "$function_name" --query 'Configuration.State' --output text 2>/dev/null || echo "UNKNOWN")
            LAST_UPDATE_STATUS=$(aws lambda get-function --function-name "$function_name" --query 'Configuration.LastUpdateStatus' --output text 2>/dev/null || echo "UNKNOWN")
            
            echo "üîç Function state: $FUNCTION_STATE, Last update status: $LAST_UPDATE_STATUS"
            
            # Check if function is ready
            if [[ "$FUNCTION_STATE" == "Active" && "$LAST_UPDATE_STATUS" == "Successful" ]]; then
              echo "‚úÖ Lambda function '$function_name' is ready for $operation_type"
              return 0
            elif [[ "$LAST_UPDATE_STATUS" == "Failed" ]]; then
              echo "‚ùå Lambda function '$function_name' update failed"
              return 1
            fi
            
            echo "‚è≥ Function not ready yet, waiting ${wait_interval}s..."
            sleep $wait_interval
            elapsed_time=$((elapsed_time + wait_interval))
          done
          
          echo "‚ö†Ô∏è  Timeout waiting for Lambda function '$function_name' to be ready"
          return 1
        }
        FUNCTIONS_EOF
        
        # Make the functions file executable
        chmod +x /tmp/stack_functions.sh
        
        echo "‚úÖ Global functions file created"

    - name: Create dynamic parameters file
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Generate secure random password if not provided
        if [[ -z "${{ secrets.LEGACY_DB_PASSWORD }}" ]]; then
          DB_PASSWORD=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-20)
          echo "‚ö†Ô∏è  Using auto-generated password (consider setting LEGACY_DB_PASSWORD secret)"
        else
          DB_PASSWORD="${{ secrets.LEGACY_DB_PASSWORD }}"
        fi
        
        # Get current timestamp for unique deployments
        CURRENT_TIME=$(date +%Y-%m-%d-%H-%M-%S)
        
        # Create parameters file
        cat > aws/parameters.json << EOF
        [
          {
            "ParameterKey": "Environment",
            "ParameterValue": "prod"
          },
          {
            "ParameterKey": "LegacyDbUsername",
            "ParameterValue": "admin"
          },
          {
            "ParameterKey": "LegacyDbPassword",
            "ParameterValue": "$DB_PASSWORD"
          },
          {
            "ParameterKey": "CurrentTime",
            "ParameterValue": "$CURRENT_TIME"
          },
          {
            "ParameterKey": "LogRetentionDays",
            "ParameterValue": "14"
          }
        ]
        EOF
        
        echo "üìã Parameters file created with timestamp: $CURRENT_TIME"

    - name: Handle workflow inputs and set environment
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Handle workflow dispatch inputs
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          [[ "${{ github.event.inputs.delete_stack_only }}" == "true" ]] && echo "DELETE_ONLY=true" >> $GITHUB_ENV
          [[ "${{ github.event.inputs.force_recreate }}" == "true" ]] && echo "FORCE_RECREATE=true" >> $GITHUB_ENV  
          [[ "${{ github.event.inputs.cleanup_all_resources }}" == "true" ]] && echo "CLEANUP_ALL=true" >> $GITHUB_ENV
          [[ "${{ github.event.inputs.skip_validation }}" == "true" ]] && echo "SKIP_VALIDATION=true" >> $GITHUB_ENV
        fi
        
        # Set deployment strategy based on trigger
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          echo "DEPLOYMENT_MODE=validation" >> $GITHUB_ENV
          echo "üîç PR mode: Will validate template only"
        else
          echo "DEPLOYMENT_MODE=full" >> $GITHUB_ENV
          echo "üöÄ Full deployment mode"
        fi

    - name: Comprehensive orphaned resource cleanup
      if: env.CLEANUP_ALL == 'true'
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Get AWS Account ID
        ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        echo "üè¢ Account ID: $ACCOUNT_ID"
        
        # Define comprehensive resource patterns
        BUCKET_PATTERNS=(
          "sub-mig-logs-$ACCOUNT_ID"
          "sub-mig-web-$ACCOUNT_ID-prod" 
          "sub-mig-web-$ACCOUNT_ID-staging"
          "sub-mig-web-$ACCOUNT_ID-dev"
          "sub-mig-data-$ACCOUNT_ID-prod"
          "sub-mig-data-$ACCOUNT_ID-staging" 
          "sub-mig-data-$ACCOUNT_ID-dev"
        )
        
        # Clean up S3 buckets
        echo "üßπ Starting comprehensive S3 bucket cleanup..."
        CLEANED_BUCKETS=0
        
        for bucket_pattern in "${BUCKET_PATTERNS[@]}"; do
          if delete_s3_bucket_enhanced "$bucket_pattern"; then
            CLEANED_BUCKETS=$((CLEANED_BUCKETS + 1))
          fi
        done
        
        echo "‚úÖ Cleaned up $CLEANED_BUCKETS bucket(s)"
        
        # Cleanup CloudFormation stacks (if cleanup_all_resources is enabled)
        if [[ "${{ env.CLEANUP_ALL }}" == "true" ]]; then
          echo "üóëÔ∏è  Comprehensive stack cleanup requested..."
          
          # Find related stacks
          RELATED_STACKS=$(aws cloudformation list-stacks --query 'StackSummaries[?contains(StackName, `subscriber-migration`) && (StackStatus == `CREATE_FAILED` || StackStatus == `ROLLBACK_COMPLETE` || StackStatus == `UPDATE_ROLLBACK_COMPLETE`)].StackName' --output text 2>/dev/null || echo "")
          
          if [[ -n "$RELATED_STACKS" && "$RELATED_STACKS" != "None" ]]; then
            echo "üóëÔ∏è  Found related failed stacks: $RELATED_STACKS"
            for stack in $RELATED_STACKS; do
              echo "üóëÔ∏è  Deleting failed stack: $stack"
              delete_failed_stack_robust "$stack" "${{ env.AWS_REGION }}" || true
            done
          else
            echo "‚ÑπÔ∏è  No failed stacks found to clean up"
          fi
        fi

    - name: Safe stack status analysis and cleanup  
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Main stack analysis and cleanup logic
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        
        echo "üîç Performing safe stack status analysis..."
        
        # Safe stack existence check
        if stack_exists "$STACK_NAME" "$AWS_REGION"; then
          CURRENT_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
          
          echo "üìä Stack exists:"
          echo "   Name: $STACK_NAME"
          echo "   Status: $CURRENT_STATUS"
          
          # Analyze stack condition
          case "$CURRENT_STATUS" in
            *_FAILED|*_ROLLBACK_COMPLETE)
              echo "üö® Stack is in failed state: $CURRENT_STATUS"
              
              # Show failure reason
              echo "üîç Analyzing failure reason..."
              aws cloudformation describe-stack-events --stack-name "$STACK_NAME" --region "$AWS_REGION" \
                --query 'StackEvents[?ResourceStatus == `CREATE_FAILED` || ResourceStatus == `UPDATE_FAILED`] | [0:5].[Timestamp,LogicalResourceId,ResourceStatusReason]' \
                --output table 2>/dev/null || echo "Could not retrieve failure details"
              
              echo "üßπ Attempting to clean up failed stack..."
              if delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION"; then
                echo "STACK_CLEANED=true" >> $GITHUB_ENV
                echo "STACK_EXISTS=false" >> $GITHUB_ENV
              else
                echo "‚ö†Ô∏è  Failed to clean up stack, but continuing..."
                echo "STACK_CLEANUP_FAILED=true" >> $GITHUB_ENV
                echo "STACK_EXISTS=true" >> $GITHUB_ENV
              fi
              ;;
            *_IN_PROGRESS)
              echo "üîÑ Stack operation in progress: $CURRENT_STATUS"
              echo "‚è≥ Will wait for current operation to complete..."
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              echo "STACK_IN_PROGRESS=true" >> $GITHUB_ENV
              ;;
            *_COMPLETE)
              echo "‚úÖ Stack is healthy: $CURRENT_STATUS"
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              ;;
            *)
              echo "‚ùì Unknown stack status: $CURRENT_STATUS"
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              ;;
          esac
        else
          echo "üìù Stack does not exist: $STACK_NAME"
          echo "‚ú® This is perfect for a clean deployment!"
          echo "STACK_EXISTS=false" >> $GITHUB_ENV
        fi

    - name: Handle special workflow modes
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Delete only mode
        if [[ "${{ env.DELETE_ONLY }}" == "true" ]]; then
          echo "üóëÔ∏è  Delete-only mode activated"
          
          if [[ "${{ env.STACK_EXISTS }}" == "true" ]]; then
            echo "üóëÔ∏è  Deleting stack: ${{ env.STACK_NAME }}"
            if delete_failed_stack_robust "${{ env.STACK_NAME }}" "${{ env.AWS_REGION }}"; then
              echo "‚úÖ Stack deleted successfully"
            else
              echo "‚ùå Failed to delete stack"
              exit 1
            fi
          else
            echo "‚ÑπÔ∏è  Stack does not exist, nothing to delete"
          fi
          
          echo "üèÅ Delete-only operation completed"
          exit 0
        fi
        
        # Force recreate mode
        if [[ "${{ env.FORCE_RECREATE }}" == "true" && "${{ env.DEPLOYMENT_MODE }}" == "full" ]]; then
          echo "üîÑ Force recreate mode activated"
          
          if [[ "${{ env.STACK_EXISTS }}" == "true" ]]; then
            echo "üóëÔ∏è  Deleting existing stack for recreation..."
            if delete_failed_stack_robust "${{ env.STACK_NAME }}" "${{ env.AWS_REGION }}"; then
              echo "‚úÖ Existing stack deleted, proceeding with creation"
              echo "STACK_EXISTS=false" >> $GITHUB_ENV
            else
              echo "‚ùå Failed to delete existing stack"
              exit 1
            fi
          else
            echo "üìù Stack doesn't exist, proceeding with creation"
          fi
        fi

    - name: Comprehensive template validation
      if: env.DEPLOYMENT_MODE == 'full' && env.SKIP_VALIDATION != 'true'
      run: |
        echo "üîç Performing comprehensive template validation..."
        
        # Basic template validation
        echo "üìã Basic CloudFormation template validation..."
        if ! aws cloudformation validate-template --template-body "file://${{ env.TEMPLATE_FILE }}" --region ${{ env.AWS_REGION }}; then
          echo "‚ùå Template validation failed"
          exit 1
        fi
        
        # Advanced template analysis
        echo "üî¨ Advanced template analysis..."
        
        # Check for circular dependencies
        echo "üîÑ Checking for circular dependencies..."
        if grep -q "DependsOn" "${{ env.TEMPLATE_FILE }}"; then
          echo "‚ö†Ô∏è  Template contains DependsOn references - review for circular dependencies"
        fi
        
        # Check resource limits
        RESOURCE_COUNT=$(grep -c "Type: AWS::" "${{ env.TEMPLATE_FILE }}" || echo "0")
        echo "üìä Template contains $RESOURCE_COUNT resources"
        
        # Validate parameter file
        if [[ -f "aws/parameters.json" ]]; then
          echo "üìã Validating parameters file..."
          if ! jq empty aws/parameters.json 2>/dev/null; then
            echo "‚ùå Invalid JSON in parameters file"
            exit 1
          fi
        fi
        
        echo "‚úÖ Template validation completed successfully"

    - name: Execute ultra-robust CloudFormation deployment
      if: env.DEPLOYMENT_MODE == 'full'
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        TEMPLATE_FILE="${{ env.TEMPLATE_FILE }}"
        PARAMETERS_FILE="aws/parameters.json"
        MAX_ATTEMPTS="${{ env.MAX_DEPLOYMENT_ATTEMPTS }}"
        TIMEOUT="${{ env.DEPLOYMENT_TIMEOUT }}"
        RETRY_DELAY="${{ env.RETRY_DELAY }}"
        
        # Determine operation type
        if [[ "${{ env.STACK_EXISTS }}" == "true" && "${{ env.FORCE_RECREATE }}" != "true" ]]; then
          OPERATION="update-stack"
          echo "üîÑ Will update existing stack..."
        else
          OPERATION="create-stack"
          echo "üÜï Will create new stack..."
        fi
        
        echo "üöÄ Starting ultra-robust CloudFormation deployment..."
        echo "   Stack: $STACK_NAME"
        echo "   Region: $AWS_REGION"
        echo "   Operation: $OPERATION"
        echo "   Max attempts: $MAX_ATTEMPTS"
        
        ATTEMPT=1
        DEPLOYMENT_SUCCESS=false
        
        while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
          echo ""
          echo "üöÄ === DEPLOYMENT ATTEMPT $ATTEMPT OF $MAX_ATTEMPTS ==="
          
          # Build deployment command
          CMD="aws cloudformation $OPERATION"
          CMD="$CMD --stack-name $STACK_NAME"
          CMD="$CMD --template-body file://$TEMPLATE_FILE"
          CMD="$CMD --capabilities CAPABILITY_NAMED_IAM CAPABILITY_IAM"
          CMD="$CMD --region $AWS_REGION"
          
          # Add parameters if file exists
          if [[ -f "$PARAMETERS_FILE" ]]; then
            CMD="$CMD --parameters file://$PARAMETERS_FILE"
          fi
          
          # Add tags
          CMD="$CMD --tags Key=Project,Value=SubscriberMigration Key=Environment,Value=Production Key=DeployedBy,Value=GitHub-Actions Key=DeploymentAttempt,Value=$ATTEMPT"
          
          echo "üéØ Executing: $CMD"
          
          # Execute deployment command
          if eval "$CMD" 2>&1 | tee "deployment-attempt-$ATTEMPT.log"; then
            echo "‚úÖ Deployment command initiated successfully"
            
            # Monitor deployment
            echo "‚è≥ Monitoring deployment progress..."
            
            # Wait for completion
            if [[ "$OPERATION" == "create-stack" ]]; then
              if timeout $TIMEOUT aws cloudformation wait stack-create-complete --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null; then
                echo "üéâ Stack creation completed successfully!"
                DEPLOYMENT_SUCCESS=true
                break
              fi
            else
              if timeout $TIMEOUT aws cloudformation wait stack-update-complete --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null; then
                echo "üéâ Stack update completed successfully!"
                DEPLOYMENT_SUCCESS=true
                break
              fi
            fi
            
            # Check final status if wait failed
            FINAL_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
            echo "üìä Final status: $FINAL_STATUS"
            
            if [[ "$FINAL_STATUS" == *"_COMPLETE" ]]; then
              DEPLOYMENT_SUCCESS=true
              break
            fi
          else
            echo "‚ùå Deployment command failed on attempt $ATTEMPT"
          fi
          
          # Prepare for retry if not at max attempts
          if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
            echo "üîÑ Preparing for retry..."
            
            # Clean up for retry
            delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION" || true
            
            # Optional cleanup on retry (avoid deleting prod buckets!)
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "‚ÑπÔ∏è Skipping destructive S3 cleanup for prod buckets on retry."
            # If you need cleanup in non-prod, uncomment and adjust names:
            # delete_s3_bucket_enhanced "sub-mig-web-$ACCOUNT_ID-dev" || true
            # delete_s3_bucket_enhanced "sub-mig-data-$ACCOUNT_ID-dev" || true
            
            # Always use create-stack for retries
            OPERATION="create-stack"
            
            echo "‚è∏Ô∏è  Waiting ${RETRY_DELAY}s before retry..."
            sleep $RETRY_DELAY
          fi
          
          ATTEMPT=$((ATTEMPT + 1))
        done
        
        # Final result
        if [[ "$DEPLOYMENT_SUCCESS" == "true" ]]; then
          echo "DEPLOYMENT_SUCCESS=true" >> $GITHUB_ENV
        else
          echo "DEPLOYMENT_SUCCESS=false" >> $GITHUB_ENV
          echo "üí• All deployment attempts failed"
          exit 1
        fi

    - name: Cache stack outputs for efficiency
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail
        
        echo "üìã Caching stack outputs to avoid repeated API calls..."
        
        # Single API call to get all outputs and save to file (using dynamic stack name)
        aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" \
          --query 'Stacks[0].Outputs' --output json > /tmp/stack-outputs.json
        
        # Debug: Show all available stack outputs
        echo "üîç Available stack outputs:"
        cat /tmp/stack-outputs.json | jq -r '.[] | "\(.OutputKey): \(.OutputValue)"'
        
        # Parse outputs once and set environment variables for all subsequent steps
        SUBSCRIBER_TABLE=$(jq -r '.[] | select(.OutputKey=="SubscriberTableName") | .OutputValue' /tmp/stack-outputs.json)
        AUDIT_TABLE=$(jq -r '.[] | select(.OutputKey=="AuditLogTableName") | .OutputValue' /tmp/stack-outputs.json)
        JOBS_TABLE=$(jq -r '.[] | select(.OutputKey=="MigrationJobsTableName") | .OutputValue' /tmp/stack-outputs.json)
        UPLOAD_BUCKET=$(jq -r '.[] | select(.OutputKey=="MigrationUploadBucketName") | .OutputValue' /tmp/stack-outputs.json)
        FRONTEND_BUCKET=$(jq -r '.[] | select(.OutputKey=="FrontendBucketName") | .OutputValue' /tmp/stack-outputs.json)
        LEGACY_DB_ENDPOINT=$(jq -r '.[] | select(.OutputKey=="LegacyDBEndpoint") | .OutputValue' /tmp/stack-outputs.json)
        LEGACY_DB_SECRET=$(jq -r '.[] | select(.OutputKey=="LegacyDBSecretArn") | .OutputValue' /tmp/stack-outputs.json)
        MIGRATION_ARN=$(jq -r '.[] | select(.OutputKey=="MigrationProcessorArn") | .OutputValue' /tmp/stack-outputs.json)
        BACKEND_API_URL=$(jq -r '.[] | select(.OutputKey=="BackendApiUrl") | .OutputValue' /tmp/stack-outputs.json)
        FRONTEND_URL=$(jq -r '.[] | select(.OutputKey=="FrontendURL") | .OutputValue' /tmp/stack-outputs.json)
        BACKEND_LAMBDA_NAME=$(jq -r '.[] | select(.OutputKey=="BackendLambdaName") | .OutputValue' /tmp/stack-outputs.json)
        MIGRATION_LAMBDA_NAME=$(jq -r '.[] | select(.OutputKey=="MigrationProcessorFunctionName") | .OutputValue' /tmp/stack-outputs.json)
        
        # Debug: Show parsed values
        echo "üîç Parsed values:"
        echo "SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE"
        echo "UPLOAD_BUCKET=$UPLOAD_BUCKET"
        echo "FRONTEND_BUCKET=$FRONTEND_BUCKET"
        echo "MIGRATION_LAMBDA_NAME=$MIGRATION_LAMBDA_NAME"
        echo "BACKEND_LAMBDA_NAME=$BACKEND_LAMBDA_NAME"
        
        # Validate critical variables
        if [[ -z "$UPLOAD_BUCKET" || "$UPLOAD_BUCKET" == "null" ]]; then
          echo "‚ùå Critical error: Upload bucket not found"
          exit 1
        fi
        
        # Verify bucket exists
        if ! aws s3api head-bucket --bucket "$UPLOAD_BUCKET" 2>/dev/null; then
          echo "‚ùå Upload bucket does not exist: $UPLOAD_BUCKET"
          exit 1
        fi
        
        echo "‚úÖ Upload bucket verified: $UPLOAD_BUCKET"
        
        # Additional validations for all critical outputs
        for VAR in FRONTEND_BUCKET MIGRATION_ARN BACKEND_API_URL FRONTEND_URL BACKEND_LAMBDA_NAME MIGRATION_LAMBDA_NAME; do
          if [[ -z "${!VAR:-}" || "${!VAR}" == "null" ]]; then
            echo "‚ùå Critical output missing: $VAR"
            exit 1
          fi
        done
        
        # Security: Mask sensitive values in GitHub Actions logs
        echo "::add-mask::$SUBSCRIBER_TABLE"
        echo "::add-mask::$AUDIT_TABLE" 
        echo "::add-mask::$JOBS_TABLE"
        echo "::add-mask::$UPLOAD_BUCKET"
        echo "::add-mask::$FRONTEND_BUCKET"
        echo "::add-mask::$LEGACY_DB_ENDPOINT"
        echo "::add-mask::$LEGACY_DB_SECRET"
        echo "::add-mask::$MIGRATION_ARN"
        echo "::add-mask::$BACKEND_API_URL"
        echo "::add-mask::$FRONTEND_URL"
        echo "::add-mask::$BACKEND_LAMBDA_NAME"
        echo "::add-mask::$MIGRATION_LAMBDA_NAME"
        
        # Export all to environment for subsequent steps
        echo "SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE" >> $GITHUB_ENV
        echo "AUDIT_TABLE=$AUDIT_TABLE" >> $GITHUB_ENV
        echo "JOBS_TABLE=$JOBS_TABLE" >> $GITHUB_ENV
        echo "UPLOAD_BUCKET=$UPLOAD_BUCKET" >> $GITHUB_ENV
        echo "FRONTEND_BUCKET=$FRONTEND_BUCKET" >> $GITHUB_ENV
        echo "LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT" >> $GITHUB_ENV
        echo "LEGACY_DB_SECRET=$LEGACY_DB_SECRET" >> $GITHUB_ENV
        echo "MIGRATION_ARN=$MIGRATION_ARN" >> $GITHUB_ENV
        echo "BACKEND_API_URL=$BACKEND_API_URL" >> $GITHUB_ENV
        echo "FRONTEND_URL=$FRONTEND_URL" >> $GITHUB_ENV
        echo "BACKEND_LAMBDA_NAME=$BACKEND_LAMBDA_NAME" >> $GITHUB_ENV
        echo "MIGRATION_LAMBDA_NAME=$MIGRATION_LAMBDA_NAME" >> $GITHUB_ENV
        
        echo "‚úÖ Stack outputs cached and environment variables set"

    - name: Deploy Lambda code automatically
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "üöÄ Deploying application code to Lambda functions..."
        
        # Backend Lambda
        if [[ -d "backend" ]]; then
          echo "üì¶ Packaging backend Lambda with dependencies..."
          cd backend
          
          # Install production dependencies if package.json exists
          if [[ -f "package.json" ]]; then
            npm ci --omit=dev
            zip -r ../backend.zip . -x "*.git*" "__tests__/*"
          elif [[ -f "requirements.txt" ]]; then
            python3 -m pip install -r requirements.txt -t .
            zip -r ../backend.zip . -x "*.git*" "__pycache__/*" "*.pyc" "venv/*"
          else
            zip -r ../backend.zip . -x "*.git*" "node_modules/*" "__pycache__/*" "*.pyc" "venv/*"
          fi
          
          cd ..
          
          aws lambda update-function-code \
            --function-name "$BACKEND_LAMBDA_NAME" \
            --zip-file fileb://backend.zip
          
          # Wait for deployment to complete
          wait_for_lambda_ready "$BACKEND_LAMBDA_NAME" "deployment"
          
          echo "‚úÖ Backend Lambda code deployed"
        else
          echo "‚ö†Ô∏è  Backend directory not found, skipping"
        fi
        
        # Migration Processor
        if [[ -d "migration-processor" ]]; then
          echo "üì¶ Packaging migration processor Lambda with dependencies..."
          cd migration-processor
          
          # Install production dependencies if requirements.txt exists
          if [[ -f "requirements.txt" ]]; then
            python3 -m pip install -r requirements.txt -t .
            zip -r ../migration.zip . -x "*.git*" "__pycache__/*" "*.pyc" "venv/*"
          elif [[ -f "package.json" ]]; then
            npm ci --omit=dev
            zip -r ../migration.zip . -x "*.git*" "__tests__/*"
          else
            zip -r ../migration.zip . -x "*.git*" "node_modules/*" "__pycache__/*" "*.pyc" "venv/*"
          fi
          
          cd ..
          
          aws lambda update-function-code \
            --function-name "$MIGRATION_LAMBDA_NAME" \
            --zip-file fileb://migration.zip
          
          # Wait for deployment to complete
          wait_for_lambda_ready "$MIGRATION_LAMBDA_NAME" "deployment"
          
          echo "‚úÖ Migration processor code deployed"
        else
          echo "‚ö†Ô∏è  Migration processor directory not found, skipping"
        fi

    - name: Configure Lambda environment variables
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "‚öôÔ∏è  Configuring Lambda environment variables..."
        
        # Wait for backend Lambda to be ready, then configure
        wait_for_lambda_ready "$BACKEND_LAMBDA_NAME" "configuration"
        aws lambda update-function-configuration \
          --function-name "$BACKEND_LAMBDA_NAME" \
          --environment "Variables={ENVIRONMENT=prod,SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE,AUDIT_TABLE=$AUDIT_TABLE,JOBS_TABLE=$JOBS_TABLE,LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT}"
        
        # Wait for migration processor to be ready, then configure
        wait_for_lambda_ready "$MIGRATION_LAMBDA_NAME" "configuration"
        aws lambda update-function-configuration \
          --function-name "$MIGRATION_LAMBDA_NAME" \
          --environment "Variables={ENVIRONMENT=prod,UPLOAD_BUCKET=$UPLOAD_BUCKET,SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE,JOBS_TABLE=$JOBS_TABLE,LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT}"
        
        echo "‚úÖ Environment variables configured"

    - name: Setup S3 notifications automatically
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail

        echo "üì° Setting up S3 ‚Üí Lambda notifications..."
        ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"

        # Get parameterized S3 filters
        PREFIX="${{ github.event.inputs.s3_prefix || 'uploads/' }}"
        SUFFIX="${{ github.event.inputs.s3_suffix || '.csv' }}"

        # Ensure bucket & Lambda are in same region
        BUCKET_REGION=$(aws s3api get-bucket-location --bucket "$UPLOAD_BUCKET" --query 'LocationConstraint' --output text)
        [[ "$BUCKET_REGION" == "None" || "$BUCKET_REGION" == "null" ]] && BUCKET_REGION="us-east-1"
        if [[ "$BUCKET_REGION" != "${{ env.AWS_REGION }}" ]]; then
          echo "‚ùå Bucket region ($BUCKET_REGION) != Lambda region (${{ env.AWS_REGION }})"
          exit 1
        fi

        # Make permission id unique per bucket to avoid ResourceConflictException on reruns
        SID="s3invoke-from-${UPLOAD_BUCKET}"

        # Remove old permission silently (ok if not present)
        aws lambda remove-permission \
          --function-name "$MIGRATION_LAMBDA_NAME" \
          --statement-id "$SID" \
          2>/dev/null || true

        # Add permission (with source-account)
        aws lambda add-permission \
          --function-name "$MIGRATION_LAMBDA_NAME" \
          --statement-id "$SID" \
          --action lambda:InvokeFunction \
          --principal s3.amazonaws.com \
          --source-arn "arn:aws:s3:::$UPLOAD_BUCKET" \
          --source-account "$ACCOUNT_ID"

        # Build notification config with prefix + suffix filter
        cat > s3notif.json <<EOF
        {
          "LambdaFunctionConfigurations": [
            {
              "Id": "MigrationProcessorTrigger",
              "LambdaFunctionArn": "$MIGRATION_ARN",
              "Events": ["s3:ObjectCreated:*"],
              "Filter": {
                "Key": {
                  "FilterRules": [
                    { "Name": "prefix", "Value": "$PREFIX" },
                    { "Name": "suffix", "Value": "$SUFFIX" }
                  ]
                }
              }
            }
          ]
        }
        EOF

        # Apply notification config
        aws s3api put-bucket-notification-configuration \
          --bucket "$UPLOAD_BUCKET" \
          --notification-configuration file://s3notif.json

        # Show final config (useful for debugging)
        aws s3api get-bucket-notification-configuration --bucket "$UPLOAD_BUCKET"

    - name: Deploy frontend automatically
      if: env.DEPLOYMENT_SUCCESS == 'true'  
      run: |
        set -euo pipefail
        
        echo "üåê Deploying React frontend..."
        
        if [[ -d "frontend" ]]; then
          cd frontend
          
          # Install dependencies and build
          if [[ -f "package.json" ]]; then
            npm ci
            npm run build
            
            # Upload to S3 (using cached environment variable)
            aws s3 sync build/ "s3://$FRONTEND_BUCKET" --delete
            
            echo "‚úÖ Frontend deployed"
            echo "üåê Frontend URL: $FRONTEND_URL"
          else
            echo "‚ö†Ô∏è  package.json not found in frontend directory"
          fi
          
          cd ..
        else
          echo "‚ö†Ô∏è  Frontend directory not found, skipping"
        fi

    - name: Initialize database schema (conditional)
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail
        echo "üóÑÔ∏è  Attempting database schema initialization..."

        # Fetch password (may require VPC access to RDS)
        SECRET_VALUE=$(aws secretsmanager get-secret-value --secret-id "$LEGACY_DB_SECRET" --query 'SecretString' --output text)
        DB_PASSWORD=$(echo "$SECRET_VALUE" | jq -r '.password')
        echo "::add-mask::$DB_PASSWORD"

        # Quick TCP check (10s) ‚Äî skip if not reachable
        if timeout 10 bash -c "cat < /dev/null > /dev/tcp/$LEGACY_DB_ENDPOINT/3306" 2>/dev/null; then
          echo "‚úÖ RDS endpoint reachable, running schema creation via mysql client"
          cat << 'EOF' > schema.sql
        CREATE DATABASE IF NOT EXISTS legacydb;
        USE legacydb;
        CREATE TABLE IF NOT EXISTS subscribers (
          id VARCHAR(64) PRIMARY KEY,
          email VARCHAR(255) NOT NULL UNIQUE,
          name VARCHAR(255),
          phone VARCHAR(20),
          status VARCHAR(50) DEFAULT 'active',
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
          INDEX idx_email (email),
          INDEX idx_status (status)
        );
        CREATE TABLE IF NOT EXISTS migration_logs (
          id VARCHAR(64) PRIMARY KEY,
          job_id VARCHAR(64),
          file_name VARCHAR(255),
          total_records INT DEFAULT 0,
          processed_count INT DEFAULT 0,
          failed_count INT DEFAULT 0,
          status VARCHAR(50) DEFAULT 'processing',
          error_message TEXT,
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          completed_at TIMESTAMP NULL,
          INDEX idx_job_id (job_id),
          INDEX idx_status (status)
        );
        EOF
          MYSQL_PWD="$DB_PASSWORD" mysql -h "$LEGACY_DB_ENDPOINT" -u admin < schema.sql
          echo "‚úÖ Database schema initialized"
        else
          echo "‚ö†Ô∏è  RDS not reachable from GitHub runner. Skipping schema init."
          echo "‚ÑπÔ∏è  Consider initializing via a Lambda task or a private runner in your VPC."
        fi

    - name: Run comprehensive smoke tests
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        set -euo pipefail
        
        echo "üß™ Running comprehensive smoke tests..."
        
        # Test API health
        echo "üîç Testing API health endpoint..."
        if curl -f -s "$BACKEND_API_URL/health" >/dev/null; then
          echo "‚úÖ API health check passed"
        else
          echo "‚ö†Ô∏è  API health check failed (may be normal for new deployments)"
        fi
        
        # Test API CORS
        echo "üîç Testing API CORS..."
        CORS_HEADERS=$(curl -s -I -X OPTIONS "$BACKEND_API_URL" | grep -i "access-control-allow")
        if [[ -n "$CORS_HEADERS" ]]; then
          echo "‚úÖ CORS headers present"
        else
          echo "‚ö†Ô∏è  CORS headers not found"
        fi
        
        # Test frontend
        echo "üîç Testing frontend availability..."
        if curl -f -s "$FRONTEND_URL" >/dev/null; then
          echo "‚úÖ Frontend accessible"
        else
          echo "‚ö†Ô∏è  Frontend not accessible yet"
        fi
        
        # Test S3 upload capability
        echo "üîç Testing S3 upload capability..."
        echo "test file" > test-upload.txt
        if aws s3 cp test-upload.txt "s3://$UPLOAD_BUCKET/test/test-upload.txt"; then
          echo "‚úÖ S3 upload test passed"
          aws s3 rm "s3://$UPLOAD_BUCKET/test/test-upload.txt" || true
        else
          echo "‚ö†Ô∏è  S3 upload test failed"
        fi
        rm -f test-upload.txt
        
        echo ""
        echo "üéâ === FULL DEPLOYMENT COMPLETED SUCCESSFULLY ==="
        echo "üåê Frontend: $FRONTEND_URL"
        echo "üì° API: $BACKEND_API_URL" 
        echo "üìÅ Upload Bucket: $UPLOAD_BUCKET"
        echo "‚úÖ All services deployed and configured automatically!"
        echo ""
        echo "üìã Next Steps:"
        echo "1. Visit the frontend URL to see your application"
        echo "2. Test file uploads through the UI"
        echo "3. Check CloudWatch logs for any issues"
        echo "4. Monitor DynamoDB tables for processed data"

    - name: Upload deployment logs
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: deployment-logs
        path: |
          deployment-attempt-*.log
          s3notif.json
          aws/parameters.json
          schema.sql
        if-no-files-found: ignore

    - name: Error reporting and cleanup
      if: failure()
      run: |
        set -euo pipefail
        
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "üí• === DEPLOYMENT WORKFLOW FAILED ==="
        
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        
        # Generate error report
        echo ""
        echo "üìã === ERROR REPORT ==="
        echo "Timestamp: $(date -u)"
        echo "Stack Name: $STACK_NAME"
        echo "Region: $AWS_REGION"
        echo "Workflow: ${{ github.workflow }}"
        echo "Run ID: ${{ github.run_id }}"
        echo ""
        
        # Check stack status safely
        STACK_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
        echo "üìä Current Stack Status: $STACK_STATUS"
        
        if [[ "$STACK_STATUS" != "STACK_NOT_EXISTS" ]]; then
          echo ""
          echo "üìã Recent Stack Events:"
          aws cloudformation describe-stack-events --stack-name "$STACK_NAME" --region "$AWS_REGION" \
            --query 'StackEvents[0:20].[Timestamp,LogicalResourceId,ResourceType,ResourceStatus,ResourceStatusReason]' \
            --output table 2>/dev/null || echo "Could not retrieve stack events"
          
          # Cleanup failed stack
          if [[ "$STACK_STATUS" == *"_FAILED" || "$STACK_STATUS" == *"_ROLLBACK_COMPLETE" ]]; then
            echo ""
            echo "üßπ Automated cleanup..."
            delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION" || true
            
            if [[ "${{ env.CLEANUP_ALL }}" == "true" ]]; then
              echo "üßπ Destructive S3 cleanup enabled (CLEANUP_ALL=true)"
              ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
              delete_s3_bucket_enhanced "sub-mig-logs-$ACCOUNT_ID" || true
              delete_s3_bucket_enhanced "sub-mig-web-$ACCOUNT_ID-prod" || true
              delete_s3_bucket_enhanced "sub-mig-data-$ACCOUNT_ID-prod" || true
            else
              echo "‚ÑπÔ∏è Skipping destructive S3 cleanup (CLEANUP_ALL!=true)"
            fi
          fi
        else
          echo "‚ÑπÔ∏è  Stack does not exist - no cleanup needed"
        fi
        
        echo ""
        echo "üíæ Error report completed"
        exit 1
