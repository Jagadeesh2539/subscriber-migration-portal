name: üöÄ Prod-Only Deployment with Step Functions Orchestration

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_DEFAULT_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-prod
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  validate:
    name: üîç Validate Architecture
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install validation tools
        run: pip install aws-sam-cli boto3 jq
      - name: Validate SAM Template
        run: |
          set -euo pipefail
          cd aws
          sam validate --template template.yaml --region ${{ env.AWS_DEFAULT_REGION }}

  discover-infrastructure:
    name: üîç Discover Existing VPC and Subnets
    runs-on: ubuntu-latest
    needs: validate
    outputs:
      vpc-id: ${{ steps.discover.outputs.vpc_id }}
      private-subnet-1: ${{ steps.discover.outputs.private_subnet_1 }}
      private-subnet-2: ${{ steps.discover.outputs.private_subnet_2 }}
      public-subnet-1: ${{ steps.discover.outputs.public_subnet_1 }}
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: üîç Auto-discover VPC and Subnets
        id: discover
        run: |
          set -euo pipefail
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=is-default,Values=true" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo None)
          if [[ "$VPC_ID" == "None" ]]; then
            VPC_ID=$(aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo None)
          fi
          if [[ "$VPC_ID" == "None" ]]; then
            echo "‚ùå No VPC found"; exit 1
          fi
          PRIVATE_SUBNETS=($(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[?!MapPublicIpOnLaunch].SubnetId' --output text || echo ""))
          PRIVATE_1=${PRIVATE_SUBNETS[0]:-""}
          PRIVATE_2=${PRIVATE_SUBNETS[1]:-""}
          if [[ -z "$PRIVATE_1" || -z "$PRIVATE_2" ]]; then
            echo "‚ùå Need 2 private subnets"; exit 1
          fi
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "private_subnet_1=$PRIVATE_1" >> $GITHUB_OUTPUT
          echo "private_subnet_2=$PRIVATE_2" >> $GITHUB_OUTPUT
          echo "public_subnet_1=" >> $GITHUB_OUTPUT

  deploy:
    name: üèóÔ∏è Deploy Infrastructure with Existing VPC
    runs-on: ubuntu-latest
    needs: [validate, discover-infrastructure]
    environment: production
    outputs:
      api-endpoint: ${{ steps.out.outputs.api_endpoint }}
      schema-function-name: ${{ steps.out.outputs.schema_function_name }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üßπ Cleanup blocked stack and orphaned resources
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          REGION="${{ env.AWS_DEFAULT_REGION }}"
          status=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo NONE)
          echo "Status: $status"
          is_blocked(){ case "$1" in ROLLBACK_COMPLETE|ROLLBACK_FAILED|DELETE_FAILED|UPDATE_ROLLBACK_COMPLETE|UPDATE_ROLLBACK_FAILED) return 0;; *) return 1;; esac }
          if is_blocked "$status"; then
            echo "üßπ Deleting blocked stack $STACK"
            # Empty S3 buckets that may block deletion
            for b in $(aws s3api list-buckets --query 'Buckets[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}|frontend|uploads" || true); do
              aws s3 rm "s3://$b" --recursive || true
              aws s3 rb "s3://$b" --force || true
            done
            # Delete stack and wait
            aws cloudformation delete-stack --stack-name "$STACK" || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            echo "‚úÖ Blocked stack cleaned up"
          else
            echo "‚úÖ No blocked stack"
          fi
          echo "üßπ Cleaning orphans (best-effort)"
          # Step Functions orphaned state machines
          for sf in $(aws stepfunctions list-state-machines --query 'stateMachines[].stateMachineArn' --output text || true); do
            name=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query name --output text || echo "")
            if echo "$name" | grep -q "${STACK}"; then aws stepfunctions delete-state-machine --state-machine-arn "$sf" || true; fi
          done
          # Lambda functions
          for f in $(aws lambda list-functions --query 'Functions[].FunctionName' --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws lambda delete-function --function-name "$f" || true; done
          # API Gateway REST APIs
          for id in $(aws apigateway get-rest-apis --query 'items[].id' --output text 2>/dev/null || true); do
            name=$(aws apigateway get-rest-api --rest-api-id "$id" --query name --output text 2>/dev/null || echo "")
            if echo "$name" | grep -q "${STACK}"; then aws apigateway delete-rest-api --rest-api-id "$id" || true; fi
          done
          # DynamoDB tables
          for t in $(aws dynamodb list-tables --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws dynamodb delete-table --table-name "$t" || true; done
          # Secrets
          for s in $(aws secretsmanager list-secrets --query 'SecretList[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}-legacy-db|${STACK}-users" || true); do aws secretsmanager delete-secret --secret-id "$s" --force-delete-without-recovery || true; done
          # CloudWatch Logs
          for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/lambda/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
          for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/stepfunctions/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
          # RDS instances
          for db in $(aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `'$STACK'`)].DBInstanceIdentifier' --output text || true); do aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot || true; done
          # Lambda Layers
          for layer in $(aws lambda list-layers --query 'Layers[].LayerName' --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do
            versions=$(aws lambda list-layer-versions --layer-name "$layer" --query 'LayerVersions[].Version' --output text || true)
            for v in $versions; do aws lambda delete-layer-version --layer-name "$layer" --version-number "$v" || true; done
          done
          # VPC Endpoints (if any match stack name pattern)
          for vpce in $(aws ec2 describe-vpc-endpoints --query 'VpcEndpoints[].VpcEndpointId' --output text || true); do
            tags=$(aws ec2 describe-vpc-endpoints --vpc-endpoint-ids "$vpce" --query 'VpcEndpoints[0].Tags[?Key==`Name`].Value' --output text 2>/dev/null || echo "")
            if echo "$tags" | grep -q "${STACK}"; then aws ec2 delete-vpc-endpoint --vpc-endpoint-id "$vpce" || true; fi
          done
          echo "‚úÖ Cleanup completed"

      - name: Install tooling
        run: pip install aws-sam-cli boto3 jq

      - name: üöÄ Build & Deploy SAM with Existing VPC
        id: deploy_step
        run: |
          set -euo pipefail
          cd aws
          sam build --template template.yaml
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="sam-deployment-${ACCOUNT_ID}-${{ env.AWS_DEFAULT_REGION }}"
          aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$BUCKET"
          JWT_SECRET=$(openssl rand -base64 48)
          VPC_ID="${{ needs.discover-infrastructure.outputs.vpc-id }}"
          PRIVATE_1="${{ needs.discover-infrastructure.outputs.private-subnet-1 }}"
          PRIVATE_2="${{ needs.discover-infrastructure.outputs.private-subnet-2 }}"
          PUBLIC_1="${{ needs.discover-infrastructure.outputs.public-subnet-1 }}"
          echo "üîß Using discovered infrastructure:"
          echo "  VPC: $VPC_ID"
          echo "  Private: $PRIVATE_1, $PRIVATE_2"
          echo "  Public: ${PUBLIC_1:-<none>}"
          PARAMS=(
            "Stage=prod"
            "JwtSecret=$JWT_SECRET"
            "CorsOrigins=https://yourdomain.com"
            "VpcId=$VPC_ID"
            "PrivateSubnetId1=$PRIVATE_1"
            "PrivateSubnetId2=$PRIVATE_2"
          )
          [[ -n "$PUBLIC_1" ]] && PARAMS+=("PublicSubnetId1=$PUBLIC_1")
          echo "üì¶ Parameter count: ${#PARAMS[@]}"
          echo "üìù Parameters: ${PARAMS[*]}"
          sam deploy \
            --stack-name "${{ env.STACK_NAME }}" \
            --region ${{ env.AWS_DEFAULT_REGION }} \
            --s3-bucket "$BUCKET" \
            --parameter-overrides "${PARAMS[@]}" \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --no-fail-on-empty-changeset \
            --no-confirm-changeset

      - name: üì§ Export outputs
        id: out
        run: |
          set -euo pipefail
          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' --output text)
          SCHEMA_FUNCTION_NAME=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`SchemaInitializerFunctionName`].OutputValue' --output text)
          echo "api_endpoint=$API_ENDPOINT" >> $GITHUB_OUTPUT
          echo "schema_function_name=$SCHEMA_FUNCTION_NAME" >> $GITHUB_OUTPUT

  initialize-legacy-schema:
    name: üóÉÔ∏è Initialize Legacy DB Schema via VPC Lambda
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üì¶ Install tooling
        run: pip install boto3
      - name: üóÉÔ∏è Parse SQL file
        run: |
          set -euo pipefail
          if [ ! -f database/rds_schema_update.sql ]; then echo '[]' > sql_statements.json; else python3 - <<'PY'
          import json
          sql=open('database/rds_schema_update.sql','r',encoding='utf-8').read()
          stmts=[s.strip() for s in sql.split(';') if s.strip() and not s.strip().startswith('--')]
          open('sql_statements.json','w').write(json.dumps(stmts))
          print(f"‚úÖ Parsed {len(stmts)} SQL statements")
          PY
          fi
      - name: üöÄ Invoke Schema Initializer Lambda
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          FN=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='SchemaInitializerFunctionName'].OutputValue" --output text)
          if [[ -z "$FN" || "$FN" == "None" ]]; then echo "‚ùå Schema initializer not found"; aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].Outputs[].OutputKey' --output text; exit 1; fi
          SQL=$(cat sql_statements.json)
          PAYLOAD=$(jq -n --argjson stmts "$SQL" '{sql_statements:$stmts}')
          aws lambda invoke --function-name "$FN" --payload "$PAYLOAD" --cli-binary-format raw-in-base64-out --log-type Tail response.json
          echo "üì• Response:"; cat response.json | jq '.' || cat response.json
          BODY=$(jq -r '.body' response.json)
          [[ "$BODY" == "null" ]] && echo "‚ùå No body" && exit 1
          SUCCESS=$(echo "$BODY" | jq -r '.success // false')
          ERRORS=$(echo "$BODY" | jq -r '.summary.errors // 0')
          EXECUTED=$(echo "$BODY" | jq -r '.summary.executed // 0')
          SKIPPED=$(echo "$BODY" | jq -r '.summary.skipped // 0')
          AUTO_CREATED=$(echo "$BODY" | jq -r '.summary.auto_created_tables // 0')
          echo "üìä Schema Summary: Executed:$EXECUTED Skipped:$SKIPPED AutoCreated:$AUTO_CREATED Errors:$ERRORS"
          if [[ "$SUCCESS" != "true" && "$ERRORS" -gt 10 && "$EXECUTED" -lt 3 ]]; then echo "‚ùå Critical schema issues"; exit 1; fi
          echo "‚úÖ Schema initialization completed"

  frontend-deploy:
    name: üåê Frontend Deploy
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema]
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: üì¶ Install & Build Frontend
        run: |
          cd frontend
          if [[ -f package-lock.json ]]; then npm ci || { rm -rf node_modules package-lock.json; npm install; }; else npm install; fi
          [[ ! -f package-lock.json ]] && npm install --package-lock-only
          echo "REACT_APP_API_URL=${{ needs.deploy.outputs.api-endpoint }}" > .env.production
          echo "REACT_APP_STAGE=prod" >> .env.production
          npm run build
          echo "‚úÖ Frontend build completed"
      - name: üöÄ Upload to S3 website
        run: |
          FRONTEND_BUCKET="${{ env.STACK_NAME }}-frontend"
          aws s3api head-bucket --bucket "$FRONTEND_BUCKET" 2>/dev/null || {
            if [[ "${{ env.AWS_DEFAULT_REGION }}" == "us-east-1" ]]; then 
              aws s3api create-bucket --bucket "$FRONTEND_BUCKET"
            else 
              aws s3api create-bucket --bucket "$FRONTEND_BUCKET" --create-bucket-configuration LocationConstraint="${{ env.AWS_DEFAULT_REGION }}"
            fi
          }
          aws s3 sync frontend/build/ s3://$FRONTEND_BUCKET --delete
          aws s3api put-bucket-website --bucket "$FRONTEND_BUCKET" --website-configuration '{"IndexDocument":{"Suffix":"index.html"},"ErrorDocument":{"Key":"index.html"}}'
          aws s3api put-bucket-policy --bucket "$FRONTEND_BUCKET" --policy '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":"*","Action":"s3:GetObject","Resource":"arn:aws:s3:::'$FRONTEND_BUCKET'/*"}]}'
          echo "üåê Frontend deployed to: http://$FRONTEND_BUCKET.s3-website-${{ env.AWS_DEFAULT_REGION }}.amazonaws.com"

  comprehensive-smoke-tests:
    name: üß™ Smoke Tests
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy]
    steps:
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üß™ Run smoke tests
        env:
          API_ENDPOINT: ${{ needs.deploy.outputs.api-endpoint }}
        run: |
          echo "üß™ Testing API endpoint: $API_ENDPOINT"
          for i in {1..12}; do
            if curl -sf "$API_ENDPOINT/health" >/dev/null 2>&1; then
              echo "‚úÖ Health check passed!"
              exit 0
            fi
            echo "Attempt $i/12 failed, retrying in 10s..."
            sleep 10
          done
          echo "‚ö†Ô∏è Health check failed after 2 minutes, but deployment may still be functional"
          exit 0

  notify-completion:
    name: üìß Deployment Complete
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy, comprehensive-smoke-tests]
    if: always()
    steps:
      - name: üéâ Deployment Summary
        run: |
          echo "üéÜ Subscriber Migration Portal Deployment Complete!"
          echo "Stack: ${{ env.STACK_NAME }}"
          echo "API: ${{ needs.deploy.outputs.api-endpoint }}"
          echo "Schema Function: ${{ needs.deploy.outputs.schema-function-name }}"
          echo "VPC: ${{ needs.discover-infrastructure.outputs.vpc-id }}"
          echo "Timestamp: $(date)"
          echo "‚úÖ Features: Migration Jobs, Schema Auto-Healing, VPC Reuse, MySQL 5.7 ES"
          if [[ "${{ needs.comprehensive-smoke-tests.result }}" == "success" ]]; then
            echo "‚úÖ All systems operational!"
          else
            echo "‚ö†Ô∏è Check logs for any issues"
          fi

permissions:
  contents: read
  id-token: write