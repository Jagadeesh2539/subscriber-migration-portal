name: üöÄ Prod-Only Deployment with Step Functions Orchestration

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_DEFAULT_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-prod
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  validate:
    name: üîç Validate Architecture
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install validation tools
        run: pip install aws-sam-cli boto3 jq
      - name: Validate SAM Template
        run: |
          set -euo pipefail
          cd aws
          sam validate --template template.yaml --region ${{ env.AWS_DEFAULT_REGION }}
      - name: Validate Step Functions Definitions
        run: |
          set -euo pipefail
          echo "üîç Validating Step Functions JSON syntax..."
          for file in aws/stepfunctions/*.json; do
            if [[ -f "$file" ]]; then
              filename=$(basename "$file")
              echo "Validating: $filename"
              
              # Validate JSON syntax
              if ! jq empty "$file" 2>/dev/null; then
                echo "‚ùå Invalid JSON syntax in $file"
                exit 1
              fi
              
              # Check required fields
              if ! jq -e '.StartAt' "$file" >/dev/null; then
                echo "‚ùå Missing StartAt field in $file"
                exit 1
              fi
              
              if ! jq -e '.States' "$file" >/dev/null; then
                echo "‚ùå Missing States field in $file"
                exit 1
              fi
              
              echo "‚úÖ Valid: $filename"
            fi
          done
          echo "üéâ All Step Functions definitions are valid!"

  deploy-stepfunctions:
    name: üì¶ Deploy Step Functions Definitions
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: Setup S3 bucket for Step Functions
        run: |
          set -euo pipefail
          UPLOADS_BUCKET="${{ env.STACK_NAME }}-uploads"
          
          # Create bucket if it doesn't exist
          if ! aws s3api head-bucket --bucket "$UPLOADS_BUCKET" 2>/dev/null; then
            echo "üöÄ Creating S3 bucket: $UPLOADS_BUCKET"
            if [[ "${{ env.AWS_DEFAULT_REGION }}" == "us-east-1" ]]; then
              aws s3api create-bucket --bucket "$UPLOADS_BUCKET"
            else
              aws s3api create-bucket \
                --bucket "$UPLOADS_BUCKET" \
                --create-bucket-configuration LocationConstraint="${{ env.AWS_DEFAULT_REGION }}"
            fi
            
            # Enable versioning and security
            aws s3api put-bucket-versioning \
              --bucket "$UPLOADS_BUCKET" \
              --versioning-configuration Status=Enabled
            
            aws s3api put-public-access-block \
              --bucket "$UPLOADS_BUCKET" \
              --public-access-block-configuration \
              "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            
            echo "‚úÖ S3 bucket created and secured: $UPLOADS_BUCKET"
          else
            echo "‚úÖ S3 bucket already exists: $UPLOADS_BUCKET"
          fi
      - name: Upload Step Functions definitions
        run: |
          set -euo pipefail
          UPLOADS_BUCKET="${{ env.STACK_NAME }}-uploads"
          
          echo "üì¶ Uploading Step Functions definitions to S3..."
          
          # Create stepfunctions directory in S3
          aws s3api put-object \
            --bucket "$UPLOADS_BUCKET" \
            --key "stepfunctions/" \
            --content-length 0
          
          # Upload each Step Function definition
          for file in aws/stepfunctions/*.json; do
            if [[ -f "$file" ]]; then
              filename=$(basename "$file")
              s3_key="stepfunctions/$filename"
              
              echo "üì§ Uploading $filename to s3://$UPLOADS_BUCKET/$s3_key"
              
              aws s3 cp "$file" "s3://$UPLOADS_BUCKET/$s3_key" \
                --content-type "application/json"
              
              echo "‚úÖ Uploaded: $filename"
            fi
          done
          
          echo "üéâ All Step Functions definitions uploaded successfully!"

  deploy:
    name: üèóÔ∏è Deploy Infrastructure with Step Functions
    runs-on: ubuntu-latest
    needs: [validate, deploy-stepfunctions]
    environment: production
    outputs:
      api-endpoint: ${{ steps.out.outputs.api_endpoint }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install tooling
        run: pip install aws-sam-cli boto3 jq

      - name: üî• Detect and cleanup failed/rollback stacks BEFORE deploy
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "NONE")
          echo "Status: $STATUS"
          is_bad() { case "$1" in ROLLBACK_*|*_ROLLBACK_*|*_FAILED|DELETE_FAILED) return 0;; *) return 1;; esac }
          cleanup() {
            echo "üßπ Cleaning resources for $STACK"
            # S3 buckets
            for b in $(aws s3api list-buckets --query 'Buckets[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}|frontend|uploads" || true); do
              aws s3 rm "s3://$b" --recursive || true; aws s3 rb "s3://$b" --force || true; done
            # Step Functions
            for sf in $(aws stepfunctions list-state-machines --query 'stateMachines[?contains(name, `'$STACK'`)].stateMachineArn' --output text || true); do
              aws stepfunctions delete-state-machine --state-machine-arn "$sf" || true; done
            # Lambda
            for f in $(aws lambda list-functions --query 'Functions[].FunctionName' --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws lambda delete-function --function-name "$f" || true; done
            # API Gateway
            for id in $(aws apigateway get-rest-apis --query 'items[?contains(name, `'$STACK'`)].id' --output text || true); do aws apigateway delete-rest-api --rest-api-id "$id" || true; done
            # DynamoDB by tag
            for t in $(aws dynamodb list-tables --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws dynamodb delete-table --table-name "$t" || true; done
            # RDS instances
            for db in $(aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `'$STACK'`)].DBInstanceIdentifier' --output text || true); do aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot || true; done
            # Logs
            for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/lambda/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
            for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/stepfunctions/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
            # Secrets
            for s in $(aws secretsmanager list-secrets --query 'SecretList[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}-legacy-db|${STACK}-users" || true); do aws secretsmanager delete-secret --secret-id "$s" --force-delete-without-recovery || true; done
          }
          if is_bad "$STATUS"; then
            echo "üóëÔ∏è Deleting bad stack $STACK"; aws cloudformation delete-stack --stack-name "$STACK" || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            cleanup
          fi

      - name: üöÄ Build & Deploy SAM with Step Functions
        id: deploy_step
        run: |
          set -euo pipefail
          cd aws
          sam build --template template.yaml
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="sam-deployment-${ACCOUNT_ID}-${{ env.AWS_DEFAULT_REGION }}"
          aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$BUCKET"
          JWT_SECRET=$(openssl rand -base64 48)
          sam deploy \
            --stack-name "${{ env.STACK_NAME }}" \
            --region ${{ env.AWS_DEFAULT_REGION }} \
            --s3-bucket "$BUCKET" \
            --parameter-overrides Stage=prod JwtSecret="$JWT_SECRET" CorsOrigins="https://yourdomain.com" \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --no-fail-on-empty-changeset \
            --no-confirm-changeset

      - name: üì§ Export API endpoint
        id: out
        run: |
          set -euo pipefail
          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' --output text)
          echo "api_endpoint=$API_ENDPOINT" >> $GITHUB_OUTPUT

  initialize-legacy-schema:
    name: üóÑÔ∏è Initialize Legacy DB Schema
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üì¶ Install DB tooling
        run: pip install boto3 pymysql
      - name: üîç Fetch legacy DB outputs
        id: cf
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          LEGACY_DB_SECRET_ARN=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='LegacyDbSecretArn'].OutputValue" --output text)
          LEGACY_DB_HOST=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='LegacyDbHost'].OutputValue" --output text)
          echo "legacy_db_secret_arn=$LEGACY_DB_SECRET_ARN" >> $GITHUB_OUTPUT
          echo "legacy_db_host=$LEGACY_DB_HOST" >> $GITHUB_OUTPUT
      - name: üóÑÔ∏è Run schema initializer
        env:
          LEGACY_DB_SECRET_ARN: ${{ steps.cf.outputs.legacy_db_secret_arn }}
          LEGACY_DB_HOST: ${{ steps.cf.outputs.legacy_db_host }}
        run: |
          set -euo pipefail
          if [ -f database/rds_schema_update.sql ]; then
            cat > run_sql.py << 'PYEOF'
            import os, json, boto3, pymysql, sys
            secret_arn = os.environ["LEGACY_DB_SECRET_ARN"]
            host = os.environ["LEGACY_DB_HOST"]
            sql_file = "database/rds_schema_update.sql"
            sm = boto3.client("secretsmanager")
            sec = json.loads(sm.get_secret_value(SecretId=secret_arn)["SecretString"])
            user = sec.get("username") or sec.get("user"); pwd = sec.get("password") or sec.get("pass")
            db  = sec.get("dbname") or sec.get("database") or ""
            conn = pymysql.connect(host=host, user=user, password=pwd, database=db, autocommit=True, connect_timeout=30)
            with conn.cursor() as cur, open(sql_file, 'r', encoding='utf-8') as f:
              sql = f.read()
              for stmt in [s.strip() for s in sql.split(';') if s.strip() and not s.strip().startswith('--')]:
                try:
                  cur.execute(stmt)
                  print(f"Executed: {stmt[:50]}...")
                except Exception as e:
                  if "already exists" not in str(e).lower():
                    print(f"Error executing {stmt[:50]}...: {e}")
            print('‚úÖ Legacy schema initialization completed')
            PYEOF
            python run_sql.py
          else
            echo "‚ùå No schema file found"; exit 1

  frontend-deploy:
    name: üåê Frontend Deploy
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema]
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: üì¶ Install & Build
        run: |
          cd frontend
          npm ci || (npm cache verify && rm -rf node_modules package-lock.json && npm install --force)
          echo "REACT_APP_API_URL=${{ needs.deploy.outputs.api-endpoint }}" > .env.production
          echo "REACT_APP_STAGE=prod" >> .env.production
          npm run build
      - name: üöÄ Upload to S3 website
        run: |
          FRONTEND_BUCKET="${{ env.STACK_NAME }}-frontend"
          aws s3api head-bucket --bucket "$FRONTEND_BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$FRONTEND_BUCKET"
          aws s3 sync frontend/build/ s3://$FRONTEND_BUCKET --delete
          aws s3api put-bucket-website --bucket "$FRONTEND_BUCKET" --website-configuration '{
            "IndexDocument": {"Suffix": "index.html"},
            "ErrorDocument": {"Key": "index.html"}
          }'

  comprehensive-smoke-tests:
    name: üß™ Comprehensive Smoke Tests with Step Functions Validation
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üì¶ Install test dependencies
        run: pip install requests pytest boto3
      - name: üß™ Run comprehensive smoke tests
        env:
          API_ENDPOINT: ${{ needs.deploy.outputs.api-endpoint }}
          STACK_NAME: ${{ env.STACK_NAME }}
        run: |
          set -euo pipefail
          cat > comprehensive_smoke_tests.py << 'PYEOF'
          import requests
          import time
          import sys
          import os
          import json
          from datetime import datetime, timedelta
          import uuid
          import boto3
          
          class ComprehensiveSmokeTests:
              def __init__(self, base_url, stack_name):
                  self.base_url = base_url.rstrip('/')
                  self.stack_name = stack_name
                  self.token = None
                  self.test_subscriber_uid = f"TEST_{uuid.uuid4().hex[:8].upper()}"
                  self.results = []
                  
                  # AWS clients for Step Functions testing
                  try:
                      self.sfn_client = boto3.client('stepfunctions')
                      self.cf_client = boto3.client('cloudformation')
                  except:
                      self.sfn_client = None
                      self.cf_client = None
              
              def log_result(self, test_name, success, message, duration=0):
                  status = "‚úÖ" if success else "‚ùå"
                  print(f"{status} {test_name}: {message} ({duration:.2f}s)")
                  self.results.append({'test': test_name, 'success': success, 'duration': duration})
              
              def test_health_endpoint(self):
                  start_time = time.time()
                  try:
                      response = requests.get(f"{self.base_url}/health", timeout=10)
                      duration = time.time() - start_time
                      if response.status_code == 200:
                          self.log_result("API Health Check", True, f"API responding (HTTP {response.status_code})", duration)
                          return True
                      self.log_result("API Health Check", False, f"Unexpected response: {response.status_code}", duration)
                      return False
                  except Exception as e:
                      duration = time.time() - start_time
                      self.log_result("API Health Check", False, f"Error: {str(e)}", duration)
                      return False
              
              def test_authentication(self):
                  start_time = time.time()
                  try:
                      response = requests.post(
                          f"{self.base_url}/auth/login",
                          json={"username": "admin", "password": "SecureAdmin123!"},
                          timeout=10
                      )
                      duration = time.time() - start_time
                      if response.status_code == 200:
                          data = response.json()
                          token = data.get('data', {}).get('token')
                          if token:
                              self.token = token
                              self.log_result("Authentication", True, "Login successful", duration)
                              return True
                      self.log_result("Authentication", False, f"Login failed: {response.status_code}", duration)
                      return False
                  except Exception as e:
                      duration = time.time() - start_time
                      self.log_result("Authentication", False, f"Error: {str(e)}", duration)
                      return False
              
              def test_step_functions_deployed(self):
                  if not self.cf_client:
                      self.log_result("Step Functions Check", False, "CloudFormation client not available", 0)
                      return False
                  
                  start_time = time.time()
                  try:
                      # Get Step Functions ARNs from CloudFormation outputs
                      response = self.cf_client.describe_stacks(StackName=self.stack_name)
                      outputs = {output['OutputKey']: output['OutputValue'] for output in response['Stacks'][0].get('Outputs', [])}
                      
                      workflows = ['MigrationWorkflowArn', 'AuditWorkflowArn', 'ExportWorkflowArn']
                      deployed_workflows = []
                      
                      for workflow in workflows:
                          if workflow in outputs:
                              deployed_workflows.append(workflow)
                              
                              # Test if Step Function is active
                              if self.sfn_client:
                                  try:
                                      sf_response = self.sfn_client.describe_state_machine(
                                          stateMachineArn=outputs[workflow]
                                      )
                                      if sf_response['status'] == 'ACTIVE':
                                          print(f"  ‚úÖ {workflow}: ACTIVE")
                                      else:
                                          print(f"  ‚ö†Ô∏è {workflow}: {sf_response['status']}")
                                  except Exception as e:
                                      print(f"  ‚ùå {workflow}: Error - {str(e)}")
                      
                      duration = time.time() - start_time
                      if len(deployed_workflows) >= 3:
                          self.log_result("Step Functions Deployment", True, f"All {len(deployed_workflows)} workflows deployed", duration)
                          return True
                      else:
                          self.log_result("Step Functions Deployment", False, f"Only {len(deployed_workflows)}/3 workflows deployed", duration)
                          return False
                  
                  except Exception as e:
                      duration = time.time() - start_time
                      self.log_result("Step Functions Deployment", False, f"Error: {str(e)}", duration)
                      return False
              
              def test_job_orchestration_endpoints(self):
                  if not self.token:
                      self.log_result("Job Orchestration", False, "No token available", 0)
                      return False
                  
                  headers = {"Authorization": f"Bearer {self.token}"}
                  
                  # Test upload endpoint
                  start_time = time.time()
                  try:
                      upload_response = requests.post(
                          f"{self.base_url}/migration/upload",
                          json={"fileName": "test.csv", "fileType": "text/csv"},
                          headers=headers,
                          timeout=15
                      )
                      
                      duration = time.time() - start_time
                      if upload_response.status_code == 200:
                          upload_data = upload_response.json()
                          if upload_data.get('data', {}).get('uploadUrl'):
                              self.log_result("Job Orchestration - Upload", True, "Upload URL generated", duration)
                              
                              # Test job creation endpoints
                              test_job_data = {
                                  "jobType": "AUDIT",
                                  "auditType": "CONSISTENCY_CHECK", 
                                  "filters": {"status": "ACTIVE"}
                              }
                              
                              start_time = time.time()
                              job_response = requests.post(
                                  f"{self.base_url}/audit/jobs",
                                  json=test_job_data,
                                  headers=headers,
                                  timeout=15
                              )
                              
                              duration = time.time() - start_time
                              if job_response.status_code == 201:
                                  job_data = job_response.json()
                                  job_id = job_data.get('data', {}).get('jobId')
                                  if job_id:
                                      self.log_result("Job Orchestration - Create", True, f"Job created: {job_id}", duration)
                                      
                                      # Test job status endpoint
                                      time.sleep(2)  # Allow job to start
                                      start_time = time.time()
                                      status_response = requests.get(
                                          f"{self.base_url}/jobs/{job_id}",
                                          headers=headers,
                                          timeout=10
                                      )
                                      
                                      duration = time.time() - start_time
                                      if status_response.status_code == 200:
                                          self.log_result("Job Orchestration - Status", True, "Job status retrieved", duration)
                                          return True
                                      else:
                                          self.log_result("Job Orchestration - Status", False, f"Status check failed: {status_response.status_code}", duration)
                                  else:
                                      self.log_result("Job Orchestration - Create", False, "No job ID returned", duration)
                              else:
                                  self.log_result("Job Orchestration - Create", False, f"Job creation failed: {job_response.status_code}", duration)
                          else:
                              self.log_result("Job Orchestration - Upload", False, "No upload URL returned", duration)
                      else:
                          self.log_result("Job Orchestration - Upload", False, f"Upload endpoint failed: {upload_response.status_code}", duration)
                      
                      return False
                  except Exception as e:
                      duration = time.time() - start_time
                      self.log_result("Job Orchestration", False, f"Error: {str(e)}", duration)
                      return False
              
              def test_settings_provisioning_mode(self):
                  if not self.token:
                      self.log_result("Settings - Get Mode", False, "No token available", 0)
                      return False
                  
                  headers = {"Authorization": f"Bearer {self.token}"}
                  
                  start_time = time.time()
                  try:
                      response = requests.get(f"{self.base_url}/settings/provisioning-mode", headers=headers, timeout=10)
                      duration = time.time() - start_time
                      if response.status_code == 200:
                          data = response.json()
                          mode = data.get('data', {}).get('mode') or data.get('mode')
                          if mode in ['CLOUD', 'LEGACY', 'DUAL_PROV']:
                              self.log_result("Settings - Get Mode", True, f"Current mode: {mode}", duration)
                              return True
                          else:
                              self.log_result("Settings - Get Mode", False, f"Invalid mode: {mode}", duration)
                              return False
                      else:
                          self.log_result("Settings - Get Mode", False, f"GET failed: {response.status_code}", duration)
                          return False
                  except Exception as e:
                      duration = time.time() - start_time
                      self.log_result("Settings - Get Mode", False, f"Error: {str(e)}", duration)
                      return False
              
              def run_all_tests(self):
                  print(f"üß™ Starting comprehensive smoke tests for: {self.base_url}")
                  print(f"üï∞Ô∏è Test started at: {datetime.now()}")
                  
                  # Wait for API to be ready
                  print("‚è≥ Waiting for API to be ready...")
                  for i in range(24):  # 4 minutes max
                      try:
                          response = requests.get(f"{self.base_url}/health", timeout=5)
                          if response.status_code == 200:
                              break
                      except:
                          pass
                      print(f"‚è≥ Retrying... ({i+1}/24)")
                      time.sleep(10)
                  
                  # Run test suite
                  test_results = []
                  
                  # Core API tests
                  test_results.append(self.test_health_endpoint())
                  test_results.append(self.test_authentication())
                  
                  # Infrastructure tests
                  test_results.append(self.test_step_functions_deployed())
                  
                  # Feature tests
                  test_results.append(self.test_settings_provisioning_mode())
                  test_results.append(self.test_job_orchestration_endpoints())
                  
                  # Summary
                  passed = sum(test_results)
                  total = len(test_results)
                  
                  print(f"\nüìä Comprehensive Test Results: {passed}/{total} tests passed")
                  print(f"üï∞Ô∏è Test completed at: {datetime.now()}")
                  
                  # Production requires high pass rate
                  if passed >= total * 0.8:  # 80% pass rate
                      print("üéâ Smoke tests passed with acceptable rate!")
                      return True
                  else:
                      print(f"‚ùå Too many test failures: {total - passed} failures")
                      return False
          
          def main():
              base_url = os.environ.get('API_ENDPOINT')
              stack_name = os.environ.get('STACK_NAME')
              if not base_url:
                  print("‚ùå API_ENDPOINT environment variable not set")
                  return False
              
              tests = ComprehensiveSmokeTests(base_url, stack_name)
              return tests.run_all_tests()
          
          if __name__ == "__main__":
              success = main()
              sys.exit(0 if success else 1)
          PYEOF
          
          python comprehensive_smoke_tests.py

  post-deployment-checks:
    name: üîç Post-Deployment System Validation
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy, comprehensive-smoke-tests]
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: üîç Validate infrastructure state
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          echo "üîç Running post-deployment infrastructure validation..."
          
          # Check stack status
          STACK_STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text)
          if [[ "$STACK_STATUS" != "CREATE_COMPLETE" && "$STACK_STATUS" != "UPDATE_COMPLETE" ]]; then
            echo "‚ùå Stack not in healthy state: $STACK_STATUS"
            exit 1
          fi
          echo "‚úÖ CloudFormation stack healthy: $STACK_STATUS"
          
          # Check Step Functions
          for sf in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::StepFunctions::StateMachine`].PhysicalResourceId' --output text); do
            STATUS=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query 'status' --output text)
            if [[ "$STATUS" != "ACTIVE" ]]; then
              echo "‚ùå Step Function $sf not active: $STATUS"
              exit 1
            fi
            NAME=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query 'name' --output text)
            echo "‚úÖ Step Function $NAME is ACTIVE"
          done
          
          # Check DynamoDB tables
          for table in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::DynamoDB::Table`].PhysicalResourceId' --output text); do
            STATUS=$(aws dynamodb describe-table --table-name "$table" --query 'Table.TableStatus' --output text)
            if [[ "$STATUS" != "ACTIVE" ]]; then
              echo "‚ùå DynamoDB table $table not active: $STATUS"
              exit 1
            fi
            echo "‚úÖ DynamoDB table $table is ACTIVE"
          done
          
          # Check RDS instance
          RDS_ID=$(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::RDS::DBInstance`].PhysicalResourceId' --output text)
          if [[ -n "$RDS_ID" ]]; then
            RDS_STATUS=$(aws rds describe-db-instances --db-instance-identifier "$RDS_ID" --query 'DBInstances[0].DBInstanceStatus' --output text)
            if [[ "$RDS_STATUS" != "available" ]]; then
              echo "‚ùå RDS instance $RDS_ID not available: $RDS_STATUS"
              exit 1
            fi
            echo "‚úÖ RDS instance $RDS_ID is available"
          fi
          
          # Check Lambda functions
          for func in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::Lambda::Function`].PhysicalResourceId' --output text); do
            STATE=$(aws lambda get-function --function-name "$func" --query 'Configuration.State' --output text)
            if [[ "$STATE" != "Active" ]]; then
              echo "‚ùå Lambda function $func not active: $STATE"
              exit 1
            fi
            echo "‚úÖ Lambda function $func is Active"
          done
          
          echo "üéâ All infrastructure components validated successfully!"

  notify-completion:
    name: üìß Notify Deployment Completion
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy, comprehensive-smoke-tests, post-deployment-checks]
    if: always()
    steps:
      - name: üéâ Deployment Summary
        run: |
          set -euo pipefail
          echo "üéÜ Deployment Summary"
          echo "==================="
          echo "Stack: ${{ env.STACK_NAME }}"
          echo "API Endpoint: ${{ needs.deploy.outputs.api-endpoint }}"
          echo "Deployment Status: ${{ needs.post-deployment-checks.result }}"
          echo "Timestamp: $(date)"
          echo ""
          
          if [[ "${{ needs.post-deployment-checks.result }}" == "success" ]]; then
            echo "‚úÖ Production deployment completed successfully!"
            echo "üéâ Your application is live and fully operational!"
            echo "üîó Features available:"
            echo "   - Settings: Provisioning mode management"
            echo "   - Cloud CRUD: DynamoDB operations"
            echo "   - Legacy CRUD: RDS MySQL operations"
            echo "   - Dual Provision: Synchronized operations"
            echo "   - Migration Jobs: Step Functions orchestration"
            echo "   - Audit Jobs: Automated consistency checks"
            echo "   - Export Jobs: Multi-system data export"
            echo "   - File Upload: S3 pre-signed URL generation"
            echo "   - Job Management: Status tracking and cancellation"
            echo "   - Comprehensive monitoring and health checks"
          else
            echo "‚ùå Deployment completed with issues - check logs above"
            echo "üõ†Ô∏è Troubleshooting:"
            echo "   1. Check CloudFormation console for stack status"
            echo "   2. Verify AWS credentials and permissions"
            echo "   3. Check Lambda function logs in CloudWatch"
            echo "   4. Validate RDS connectivity and schema"
            echo "   5. Check Step Functions execution status"
          fi

permissions:
  contents: read
  id-token: write