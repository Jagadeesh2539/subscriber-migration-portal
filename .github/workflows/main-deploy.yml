name: Deploy CloudFormation Stack with Ultra-Robust Auto-Management

on:
  push:
    branches: [ main ]
    paths:
      - 'aws/**'
      - '.github/workflows/**'
      - 'backend/**'
      - 'migration-processor/**'
      - 'frontend/**'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      delete_stack_only:
        description: 'Delete stack only (true/false)'
        required: false
        default: 'false'
      force_recreate:
        description: 'Force recreate stack (true/false)'
        required: false
        default: 'false'
      cleanup_all_resources:
        description: 'Cleanup all orphaned resources (true/false)'
        required: false
        default: 'false'
      skip_validation:
        description: 'Skip template validation (true/false)'
        required: false
        default: 'false'

env:
  AWS_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-main
  TEMPLATE_FILE: aws/cloudformation.yaml
  MAX_DEPLOYMENT_ATTEMPTS: 5
  DEPLOYMENT_TIMEOUT: 2400
  RETRY_DELAY: 120

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install and configure AWS CLI
      run: |
        # Install AWS CLI v2
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip -q awscliv2.zip
        sudo ./aws/install --update
        
        # Install additional tools
        sudo apt-get update
        sudo apt-get install -y jq mysql-client
        curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
        sudo apt-get install -y nodejs
        
        # Verify installation
        aws --version
        jq --version
        node --version
        npm --version
        
        # Configure AWS CLI for better performance
        aws configure set max_concurrent_requests 20
        aws configure set max_queue_size 10000
        aws configure set region ${{ env.AWS_REGION }}
        aws configure set output json

    - name: Verify AWS credentials and permissions
      run: |
        echo "🔐 Testing AWS credentials..."
        
        # Get caller identity
        CALLER_IDENTITY=$(aws sts get-caller-identity)
        echo "✅ Caller Identity: $CALLER_IDENTITY"
        
        # Test CloudFormation permissions
        echo "🔍 Testing CloudFormation permissions..."
        aws cloudformation list-stacks --query 'StackSummaries[0]' --output table || echo "⚠️  Limited CloudFormation permissions"
        
        # Test S3 permissions
        echo "🪣 Testing S3 permissions..."
        aws s3 ls || echo "⚠️  Limited S3 permissions"

    - name: Create global functions file
      run: |
        # Create a global functions file that can be sourced by all steps
        cat > /tmp/stack_functions.sh << 'FUNCTIONS_EOF'
        #!/bin/bash
        
        # Enhanced stack existence check (FIXED: added || false to prevent step termination)
        stack_exists() {
          aws cloudformation describe-stacks --stack-name "$1" --region "$2" &>/dev/null || false
        }
        
        # Enhanced stack status retrieval
        get_stack_status() {
          if aws cloudformation describe-stacks --stack-name "$1" --region "$2" &>/dev/null; then
            aws cloudformation describe-stacks --stack-name "$1" --region "$2" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "UNKNOWN"
          else
            echo "STACK_NOT_EXISTS"
          fi
        }
        
        # Unified Lambda readiness check function (consolidates both wait functions)
        wait_for_lambda_ready() {
          local function_name="$1"
          local operation_type="${2:-deployment}"  # 'deployment' or 'configuration'
          local max_wait=300  # 5 minutes
          local wait_time=0
          
          echo "⏳ Waiting for Lambda $operation_type to complete: $function_name"
          
          while [[ $wait_time -lt $max_wait ]]; do
            local state=$(aws lambda get-function --function-name "$function_name" --query 'Configuration.State' --output text 2>/dev/null || echo "Unknown")
            local status=$(aws lambda get-function --function-name "$function_name" --query 'Configuration.LastUpdateStatus' --output text 2>/dev/null || echo "Unknown")
            
            # Success condition
            if [[ "$state" == "Active" && "$status" == "Successful" ]]; then
              echo "✅ Lambda $operation_type completed: $function_name"
              return 0
            fi
            
            # Failure condition
            if [[ "$status" == "Failed" ]]; then
              echo "❌ Lambda $operation_type failed: $function_name"
              return 1
            fi
            
            # Progress logging (every 30 seconds)
            if [[ $((wait_time % 30)) -eq 0 ]]; then
              echo "⏳ Still waiting for $operation_type... ($wait_time/$max_wait seconds) - State: $state, Status: $status"
            fi
            
            sleep 10
            wait_time=$((wait_time + 10))
          done
          
          echo "⚠️  Timeout waiting for Lambda $operation_type: $function_name"
          return 1
        }
        
        # Enhanced S3 bucket deletion function
        delete_s3_bucket_enhanced() {
          local bucket_name="$1"
          local max_retries=3
          local retry=1
          
          # Check if bucket exists
          if ! aws s3api head-bucket --bucket "$bucket_name" 2>/dev/null; then
            echo "ℹ️  Bucket does not exist: $bucket_name"
            return 0
          fi
          
          echo "🗑️  Processing bucket: $bucket_name"
          
          while [[ $retry -le $max_retries ]]; do
            echo "🔄 Attempt $retry of $max_retries for bucket: $bucket_name"
            
            # Get bucket region
            BUCKET_REGION=$(aws s3api get-bucket-location --bucket "$bucket_name" --query 'LocationConstraint' --output text 2>/dev/null || echo "us-east-1")
            [[ "$BUCKET_REGION" == "None" || "$BUCKET_REGION" == "null" ]] && BUCKET_REGION="us-east-1"
            
            # Check bucket versioning
            VERSIONING=$(aws s3api get-bucket-versioning --bucket "$bucket_name" --region "$BUCKET_REGION" --query 'Status' --output text 2>/dev/null || echo "None")
            
            # Handle versioned buckets
            if [[ "$VERSIONING" == "Enabled" ]]; then
              echo "📦 Removing all versions from versioned bucket..."
              
              # Delete all object versions
              aws s3api list-object-versions --bucket "$bucket_name" --region "$BUCKET_REGION" \
                --query 'Versions[].{Key:Key,VersionId:VersionId}' --output text 2>/dev/null | \
              while read -r key version_id; do
                if [[ -n "$key" && -n "$version_id" && "$key" != "None" && "$version_id" != "None" ]]; then
                  aws s3api delete-object --bucket "$bucket_name" --key "$key" --version-id "$version_id" --region "$BUCKET_REGION" 2>/dev/null || true
                fi
              done
              
              # Delete all delete markers
              aws s3api list-object-versions --bucket "$bucket_name" --region "$BUCKET_REGION" \
                --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' --output text 2>/dev/null | \
              while read -r key version_id; do
                if [[ -n "$key" && -n "$version_id" && "$key" != "None" && "$version_id" != "None" ]]; then
                  aws s3api delete-object --bucket "$bucket_name" --key "$key" --version-id "$version_id" --region "$BUCKET_REGION" 2>/dev/null || true
                fi
              done
            fi
            
            # Remove all objects
            echo "🧹 Removing all objects..."
            aws s3 rm "s3://$bucket_name" --recursive --region "$BUCKET_REGION" 2>/dev/null || true
            
            # Remove bucket configurations
            aws s3api delete-bucket-policy --bucket "$bucket_name" --region "$BUCKET_REGION" 2>/dev/null || true
            aws s3api put-bucket-notification-configuration --bucket "$bucket_name" --notification-configuration "{}" --region "$BUCKET_REGION" 2>/dev/null || true
            
            # Delete the bucket
            if aws s3api delete-bucket --bucket "$bucket_name" --region "$BUCKET_REGION" 2>/dev/null; then
              echo "✅ Bucket deleted successfully: $bucket_name"
              return 0
            else
              echo "⚠️  Failed to delete bucket on attempt $retry: $bucket_name"
              retry=$((retry + 1))
              [[ $retry -le $max_retries ]] && sleep 10
            fi
          done
          
          echo "❌ Failed to delete bucket after $max_retries attempts: $bucket_name"
          return 1
        }
        
        # Enhanced stack deletion with retry logic
        delete_failed_stack_robust() {
          local stack_name="$1"
          local region="$2"
          local max_retries=3
          local retry=1
          
          echo "🗑️  Initiating robust stack deletion: $stack_name"
          
          # First check if stack actually exists
          if ! stack_exists "$stack_name" "$region"; then
            echo "ℹ️  Stack does not exist: $stack_name"
            return 0
          fi
          
          while [[ $retry -le $max_retries ]]; do
            echo "🔄 Deletion attempt $retry of $max_retries"
            
            # Get current status
            local current_status=$(get_stack_status "$stack_name" "$region")
            echo "📊 Current status: $current_status"
            
            # Handle different statuses
            case "$current_status" in
              DELETE_COMPLETE|STACK_NOT_EXISTS)
                echo "✅ Stack deletion completed"
                return 0
                ;;
              DELETE_IN_PROGRESS)
                echo "⏳ Stack deletion already in progress, waiting..."
                ;;
              *)
                echo "🗑️  Initiating stack deletion..."
                aws cloudformation delete-stack --stack-name "$stack_name" --region "$region" 2>/dev/null || true
                ;;
            esac
            
            # Wait for deletion with timeout
            echo "⏳ Waiting for stack deletion (timeout: 30 minutes)..."
            if timeout 1800 aws cloudformation wait stack-delete-complete --stack-name "$stack_name" --region "$region" 2>/dev/null; then
              echo "✅ Stack deleted successfully: $stack_name"
              return 0
            else
              echo "⚠️  Stack deletion timeout or failed on attempt $retry"
              retry=$((retry + 1))
              [[ $retry -le $max_retries ]] && sleep 60
            fi
          done
          
          echo "❌ Failed to delete stack after $max_retries attempts: $stack_name"
          return 1
        }
        FUNCTIONS_EOF
        
        # Make the functions file executable
        chmod +x /tmp/stack_functions.sh
        
        echo "✅ Global functions file created"

    - name: Create dynamic parameters file
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Generate secure random password if not provided
        if [[ -z "${{ secrets.LEGACY_DB_PASSWORD }}" ]]; then
          DB_PASSWORD=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-20)
          echo "⚠️  Using auto-generated password (consider setting LEGACY_DB_PASSWORD secret)"
        else
          DB_PASSWORD="${{ secrets.LEGACY_DB_PASSWORD }}"
        fi
        
        # Get current timestamp for unique deployments
        CURRENT_TIME=$(date +%Y-%m-%d-%H-%M-%S)
        
        # Create parameters file
        cat > aws/parameters.json << EOF
        [
          {
            "ParameterKey": "Environment",
            "ParameterValue": "prod"
          },
          {
            "ParameterKey": "LegacyDbUsername",
            "ParameterValue": "admin"
          },
          {
            "ParameterKey": "LegacyDbPassword",
            "ParameterValue": "$DB_PASSWORD"
          },
          {
            "ParameterKey": "CurrentTime",
            "ParameterValue": "$CURRENT_TIME"
          },
          {
            "ParameterKey": "LogRetentionDays",
            "ParameterValue": "14"
          }
        ]
        EOF
        
        echo "📋 Parameters file created with timestamp: $CURRENT_TIME"

    - name: Handle workflow inputs and set environment
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Handle workflow dispatch inputs
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          [[ "${{ github.event.inputs.delete_stack_only }}" == "true" ]] && echo "DELETE_ONLY=true" >> $GITHUB_ENV
          [[ "${{ github.event.inputs.force_recreate }}" == "true" ]] && echo "FORCE_RECREATE=true" >> $GITHUB_ENV  
          [[ "${{ github.event.inputs.cleanup_all_resources }}" == "true" ]] && echo "CLEANUP_ALL=true" >> $GITHUB_ENV
          [[ "${{ github.event.inputs.skip_validation }}" == "true" ]] && echo "SKIP_VALIDATION=true" >> $GITHUB_ENV
        fi
        
        # Set deployment strategy based on trigger
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          echo "DEPLOYMENT_MODE=validation" >> $GITHUB_ENV
          echo "🔍 PR mode: Will validate template only"
        else
          echo "DEPLOYMENT_MODE=full" >> $GITHUB_ENV
          echo "🚀 Full deployment mode"
        fi

    - name: Comprehensive orphaned resource cleanup
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Get AWS Account ID
        ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        echo "🏢 Account ID: $ACCOUNT_ID"
        
        # Define comprehensive resource patterns
        BUCKET_PATTERNS=(
          "sub-mig-logs-$ACCOUNT_ID"
          "sub-mig-web-$ACCOUNT_ID-prod" 
          "sub-mig-web-$ACCOUNT_ID-staging"
          "sub-mig-web-$ACCOUNT_ID-dev"
          "sub-mig-data-$ACCOUNT_ID-prod"
          "sub-mig-data-$ACCOUNT_ID-staging" 
          "sub-mig-data-$ACCOUNT_ID-dev"
        )
        
        # Clean up S3 buckets
        echo "🧹 Starting comprehensive S3 bucket cleanup..."
        CLEANED_BUCKETS=0
        
        for bucket_pattern in "${BUCKET_PATTERNS[@]}"; do
          if delete_s3_bucket_enhanced "$bucket_pattern"; then
            CLEANED_BUCKETS=$((CLEANED_BUCKETS + 1))
          fi
        done
        
        echo "✅ Cleaned up $CLEANED_BUCKETS bucket(s)"
        
        # Cleanup CloudFormation stacks (if cleanup_all_resources is enabled)
        if [[ "${{ env.CLEANUP_ALL }}" == "true" ]]; then
          echo "🗑️  Comprehensive stack cleanup requested..."
          
          # Find related stacks
          RELATED_STACKS=$(aws cloudformation list-stacks --query 'StackSummaries[?contains(StackName, `subscriber-migration`) && (StackStatus == `CREATE_FAILED` || StackStatus == `ROLLBACK_COMPLETE` || StackStatus == `UPDATE_ROLLBACK_COMPLETE`)].StackName' --output text 2>/dev/null || echo "")
          
          if [[ -n "$RELATED_STACKS" && "$RELATED_STACKS" != "None" ]]; then
            echo "🗑️  Found related failed stacks: $RELATED_STACKS"
            for stack in $RELATED_STACKS; do
              echo "🗑️  Deleting failed stack: $stack"
              delete_failed_stack_robust "$stack" "${{ env.AWS_REGION }}" || true
            done
          else
            echo "ℹ️  No failed stacks found to clean up"
          fi
        fi

    - name: Safe stack status analysis and cleanup  
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Main stack analysis and cleanup logic
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        
        echo "🔍 Performing safe stack status analysis..."
        
        # Safe stack existence check
        if stack_exists "$STACK_NAME" "$AWS_REGION"; then
          CURRENT_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
          
          echo "📊 Stack exists:"
          echo "   Name: $STACK_NAME"
          echo "   Status: $CURRENT_STATUS"
          
          # Analyze stack condition
          case "$CURRENT_STATUS" in
            *_FAILED|*_ROLLBACK_COMPLETE)
              echo "🚨 Stack is in failed state: $CURRENT_STATUS"
              
              # Show failure reason
              echo "🔍 Analyzing failure reason..."
              aws cloudformation describe-stack-events --stack-name "$STACK_NAME" --region "$AWS_REGION" \
                --query 'StackEvents[?ResourceStatus == `CREATE_FAILED` || ResourceStatus == `UPDATE_FAILED`] | [0:5].[Timestamp,LogicalResourceId,ResourceStatusReason]' \
                --output table 2>/dev/null || echo "Could not retrieve failure details"
              
              echo "🧹 Attempting to clean up failed stack..."
              if delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION"; then
                echo "STACK_CLEANED=true" >> $GITHUB_ENV
                echo "STACK_EXISTS=false" >> $GITHUB_ENV
              else
                echo "⚠️  Failed to clean up stack, but continuing..."
                echo "STACK_CLEANUP_FAILED=true" >> $GITHUB_ENV
                echo "STACK_EXISTS=true" >> $GITHUB_ENV
              fi
              ;;
            *_IN_PROGRESS)
              echo "🔄 Stack operation in progress: $CURRENT_STATUS"
              echo "⏳ Will wait for current operation to complete..."
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              echo "STACK_IN_PROGRESS=true" >> $GITHUB_ENV
              ;;
            *_COMPLETE)
              echo "✅ Stack is healthy: $CURRENT_STATUS"
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              ;;
            *)
              echo "❓ Unknown stack status: $CURRENT_STATUS"
              echo "STACK_EXISTS=true" >> $GITHUB_ENV
              ;;
          esac
        else
          echo "📝 Stack does not exist: $STACK_NAME"
          echo "✨ This is perfect for a clean deployment!"
          echo "STACK_EXISTS=false" >> $GITHUB_ENV
        fi

    - name: Handle special workflow modes
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        # Delete only mode
        if [[ "${{ env.DELETE_ONLY }}" == "true" ]]; then
          echo "🗑️  Delete-only mode activated"
          
          if [[ "${{ env.STACK_EXISTS }}" == "true" ]]; then
            echo "🗑️  Deleting stack: ${{ env.STACK_NAME }}"
            if delete_failed_stack_robust "${{ env.STACK_NAME }}" "${{ env.AWS_REGION }}"; then
              echo "✅ Stack deleted successfully"
            else
              echo "❌ Failed to delete stack"
              exit 1
            fi
          else
            echo "ℹ️  Stack does not exist, nothing to delete"
          fi
          
          echo "🏁 Delete-only operation completed"
          exit 0
        fi
        
        # Force recreate mode
        if [[ "${{ env.FORCE_RECREATE }}" == "true" && "${{ env.DEPLOYMENT_MODE }}" == "full" ]]; then
          echo "🔄 Force recreate mode activated"
          
          if [[ "${{ env.STACK_EXISTS }}" == "true" ]]; then
            echo "🗑️  Deleting existing stack for recreation..."
            if delete_failed_stack_robust "${{ env.STACK_NAME }}" "${{ env.AWS_REGION }}"; then
              echo "✅ Existing stack deleted, proceeding with creation"
              echo "STACK_EXISTS=false" >> $GITHUB_ENV
            else
              echo "❌ Failed to delete existing stack"
              exit 1
            fi
          else
            echo "📝 Stack doesn't exist, proceeding with creation"
          fi
        fi

    - name: Comprehensive template validation
      if: env.DEPLOYMENT_MODE == 'full' && env.SKIP_VALIDATION != 'true'
      run: |
        echo "🔍 Performing comprehensive template validation..."
        
        # Basic template validation
        echo "📋 Basic CloudFormation template validation..."
        if ! aws cloudformation validate-template --template-body "file://${{ env.TEMPLATE_FILE }}" --region ${{ env.AWS_REGION }}; then
          echo "❌ Template validation failed"
          exit 1
        fi
        
        # Advanced template analysis
        echo "🔬 Advanced template analysis..."
        
        # Check for circular dependencies
        echo "🔄 Checking for circular dependencies..."
        if grep -q "DependsOn" "${{ env.TEMPLATE_FILE }}"; then
          echo "⚠️  Template contains DependsOn references - review for circular dependencies"
        fi
        
        # Check resource limits
        RESOURCE_COUNT=$(grep -c "Type: AWS::" "${{ env.TEMPLATE_FILE }}" || echo "0")
        echo "📊 Template contains $RESOURCE_COUNT resources"
        
        # Validate parameter file
        if [[ -f "aws/parameters.json" ]]; then
          echo "📋 Validating parameters file..."
          if ! jq empty aws/parameters.json 2>/dev/null; then
            echo "❌ Invalid JSON in parameters file"
            exit 1
          fi
        fi
        
        echo "✅ Template validation completed successfully"

    - name: Execute ultra-robust CloudFormation deployment
      if: env.DEPLOYMENT_MODE == 'full'
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        TEMPLATE_FILE="${{ env.TEMPLATE_FILE }}"
        PARAMETERS_FILE="aws/parameters.json"
        MAX_ATTEMPTS="${{ env.MAX_DEPLOYMENT_ATTEMPTS }}"
        TIMEOUT="${{ env.DEPLOYMENT_TIMEOUT }}"
        RETRY_DELAY="${{ env.RETRY_DELAY }}"
        
        # Determine operation type
        if [[ "${{ env.STACK_EXISTS }}" == "true" && "${{ env.FORCE_RECREATE }}" != "true" ]]; then
          OPERATION="update-stack"
          echo "🔄 Will update existing stack..."
        else
          OPERATION="create-stack"
          echo "🆕 Will create new stack..."
        fi
        
        echo "🚀 Starting ultra-robust CloudFormation deployment..."
        echo "   Stack: $STACK_NAME"
        echo "   Region: $AWS_REGION"
        echo "   Operation: $OPERATION"
        echo "   Max attempts: $MAX_ATTEMPTS"
        
        ATTEMPT=1
        DEPLOYMENT_SUCCESS=false
        
        while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
          echo ""
          echo "🚀 === DEPLOYMENT ATTEMPT $ATTEMPT OF $MAX_ATTEMPTS ==="
          
          # Build deployment command
          CMD="aws cloudformation $OPERATION"
          CMD="$CMD --stack-name $STACK_NAME"
          CMD="$CMD --template-body file://$TEMPLATE_FILE"
          CMD="$CMD --capabilities CAPABILITY_NAMED_IAM CAPABILITY_IAM"
          CMD="$CMD --region $AWS_REGION"
          
          # Add parameters if file exists
          if [[ -f "$PARAMETERS_FILE" ]]; then
            CMD="$CMD --parameters file://$PARAMETERS_FILE"
          fi
          
          # Add tags
          CMD="$CMD --tags Key=Project,Value=SubscriberMigration Key=Environment,Value=Production Key=DeployedBy,Value=GitHub-Actions Key=DeploymentAttempt,Value=$ATTEMPT"
          
          echo "🎯 Executing: $CMD"
          
          # Execute deployment command
          if eval "$CMD" 2>&1 | tee "deployment-attempt-$ATTEMPT.log"; then
            echo "✅ Deployment command initiated successfully"
            
            # Monitor deployment
            echo "⏳ Monitoring deployment progress..."
            
            # Wait for completion
            if [[ "$OPERATION" == "create-stack" ]]; then
              if timeout $TIMEOUT aws cloudformation wait stack-create-complete --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null; then
                echo "🎉 Stack creation completed successfully!"
                DEPLOYMENT_SUCCESS=true
                break
              fi
            else
              if timeout $TIMEOUT aws cloudformation wait stack-update-complete --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null; then
                echo "🎉 Stack update completed successfully!"
                DEPLOYMENT_SUCCESS=true
                break
              fi
            fi
            
            # Check final status if wait failed
            FINAL_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
            echo "📊 Final status: $FINAL_STATUS"
            
            if [[ "$FINAL_STATUS" == *"_COMPLETE" ]]; then
              DEPLOYMENT_SUCCESS=true
              break
            fi
          else
            echo "❌ Deployment command failed on attempt $ATTEMPT"
          fi
          
          # Prepare for retry if not at max attempts
          if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
            echo "🔄 Preparing for retry..."
            
            # Clean up for retry
            delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION" || true
            
            # Clean up S3 buckets
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            delete_s3_bucket_enhanced "sub-mig-logs-$ACCOUNT_ID" || true
            delete_s3_bucket_enhanced "sub-mig-web-$ACCOUNT_ID-prod" || true
            delete_s3_bucket_enhanced "sub-mig-data-$ACCOUNT_ID-prod" || true
            
            # Always use create-stack for retries
            OPERATION="create-stack"
            
            echo "⏸️  Waiting ${RETRY_DELAY}s before retry..."
            sleep $RETRY_DELAY
          fi
          
          ATTEMPT=$((ATTEMPT + 1))
        done
        
        # Final result
        if [[ "$DEPLOYMENT_SUCCESS" == "true" ]]; then
          echo "DEPLOYMENT_SUCCESS=true" >> $GITHUB_ENV
        else
          echo "DEPLOYMENT_SUCCESS=false" >> $GITHUB_ENV
          echo "💥 All deployment attempts failed"
          exit 1
        fi

    - name: Cache stack outputs for efficiency
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        echo "📋 Caching stack outputs to avoid repeated API calls..."
        
        # Single API call to get all outputs and save to file (using dynamic stack name)
        aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" \
          --query 'Stacks[0].Outputs' --output json > /tmp/stack-outputs.json
        
        # Parse outputs once and set environment variables for all subsequent steps
        SUBSCRIBER_TABLE=$(jq -r '.[] | select(.OutputKey=="SubscriberTableName") | .OutputValue' /tmp/stack-outputs.json)
        AUDIT_TABLE=$(jq -r '.[] | select(.OutputKey=="AuditLogTableName") | .OutputValue' /tmp/stack-outputs.json)
        JOBS_TABLE=$(jq -r '.[] | select(.OutputKey=="MigrationJobsTableName") | .OutputValue' /tmp/stack-outputs.json)
        UPLOAD_BUCKET=$(jq -r '.[] | select(.OutputKey=="MigrationUploadBucketName") | .OutputValue' /tmp/stack-outputs.json)
        FRONTEND_BUCKET=$(jq -r '.[] | select(.OutputKey=="FrontendBucketName") | .OutputValue' /tmp/stack-outputs.json)
        LEGACY_DB_ENDPOINT=$(jq -r '.[] | select(.OutputKey=="LegacyDBEndpoint") | .OutputValue' /tmp/stack-outputs.json)
        LEGACY_DB_SECRET=$(jq -r '.[] | select(.OutputKey=="LegacyDBSecretArn") | .OutputValue' /tmp/stack-outputs.json)
        MIGRATION_ARN=$(jq -r '.[] | select(.OutputKey=="MigrationProcessorArn") | .OutputValue' /tmp/stack-outputs.json)
        BACKEND_API_URL=$(jq -r '.[] | select(.OutputKey=="BackendApiUrl") | .OutputValue' /tmp/stack-outputs.json)
        FRONTEND_URL=$(jq -r '.[] | select(.OutputKey=="FrontendURL") | .OutputValue' /tmp/stack-outputs.json)
        BACKEND_LAMBDA_NAME=$(jq -r '.[] | select(.OutputKey=="BackendLambdaName") | .OutputValue' /tmp/stack-outputs.json)
        MIGRATION_LAMBDA_NAME=$(jq -r '.[] | select(.OutputKey=="MigrationProcessorFunctionName") | .OutputValue' /tmp/stack-outputs.json)
        
        # Security: Mask sensitive values in GitHub Actions logs
        echo "::add-mask::$SUBSCRIBER_TABLE"
        echo "::add-mask::$AUDIT_TABLE" 
        echo "::add-mask::$JOBS_TABLE"
        echo "::add-mask::$UPLOAD_BUCKET"
        echo "::add-mask::$FRONTEND_BUCKET"
        echo "::add-mask::$LEGACY_DB_ENDPOINT"
        echo "::add-mask::$LEGACY_DB_SECRET"
        echo "::add-mask::$MIGRATION_ARN"
        echo "::add-mask::$BACKEND_API_URL"
        echo "::add-mask::$FRONTEND_URL"
        echo "::add-mask::$BACKEND_LAMBDA_NAME"
        echo "::add-mask::$MIGRATION_LAMBDA_NAME"
        
        # Export all to environment for subsequent steps
        echo "SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE" >> $GITHUB_ENV
        echo "AUDIT_TABLE=$AUDIT_TABLE" >> $GITHUB_ENV
        echo "JOBS_TABLE=$JOBS_TABLE" >> $GITHUB_ENV
        echo "UPLOAD_BUCKET=$UPLOAD_BUCKET" >> $GITHUB_ENV
        echo "FRONTEND_BUCKET=$FRONTEND_BUCKET" >> $GITHUB_ENV
        echo "LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT" >> $GITHUB_ENV
        echo "LEGACY_DB_SECRET=$LEGACY_DB_SECRET" >> $GITHUB_ENV
        echo "MIGRATION_ARN=$MIGRATION_ARN" >> $GITHUB_ENV
        echo "BACKEND_API_URL=$BACKEND_API_URL" >> $GITHUB_ENV
        echo "FRONTEND_URL=$FRONTEND_URL" >> $GITHUB_ENV
        echo "BACKEND_LAMBDA_NAME=$BACKEND_LAMBDA_NAME" >> $GITHUB_ENV
        echo "MIGRATION_LAMBDA_NAME=$MIGRATION_LAMBDA_NAME" >> $GITHUB_ENV
        
        echo "✅ Stack outputs cached and environment variables set"

    - name: Deploy Lambda code with smart directory detection
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "🚀 Deploying application code to Lambda functions..."
        
        # Backend Lambda
        if [[ -d "backend" ]]; then
          echo "📦 Packaging backend Lambda..."
          cd backend
          zip -r ../backend.zip . -x "*.git*" "node_modules/*" "__pycache__/*" "*.pyc" "venv/*" "*.DS_Store"
          cd ..
          
          if [[ -n "$BACKEND_LAMBDA_NAME" && "$BACKEND_LAMBDA_NAME" != "null" ]]; then
            echo "🚀 Deploying backend Lambda code..."
            aws lambda update-function-code \
              --function-name "$BACKEND_LAMBDA_NAME" \
              --zip-file fileb://backend.zip
            
            # Wait for deployment to complete
            if wait_for_lambda_ready "$BACKEND_LAMBDA_NAME" "deployment"; then
              echo "✅ Backend Lambda code deployed successfully"
            else
              echo "⚠️  Backend Lambda deployment may have issues, but continuing..."
            fi
          else
            echo "⚠️  Backend Lambda name not found"
          fi
        else
          echo "⚠️  Backend directory not found, skipping"
        fi
        
        # Migration Processor - check multiple possible locations
        MIGRATION_DIR=""
        for dir in "migration-processor" "migrationprocessor" "processor" "backend/migration-processor" "backend/processor"; do
          if [[ -d "$dir" ]]; then
            MIGRATION_DIR="$dir"
            break
          fi
        done
        
        if [[ -n "$MIGRATION_DIR" ]]; then
          echo "📦 Packaging migration processor Lambda from: $MIGRATION_DIR"
          cd "$MIGRATION_DIR"
          zip -r ../migration.zip . -x "*.git*" "node_modules/*" "__pycache__/*" "*.pyc" "venv/*" "*.DS_Store"
          cd ..
          
          if [[ -n "$MIGRATION_LAMBDA_NAME" && "$MIGRATION_LAMBDA_NAME" != "null" ]]; then
            echo "🚀 Deploying migration processor Lambda code..."
            aws lambda update-function-code \
              --function-name "$MIGRATION_LAMBDA_NAME" \
              --zip-file fileb://migration.zip
            
            # Wait for deployment to complete
            if wait_for_lambda_ready "$MIGRATION_LAMBDA_NAME" "deployment"; then
              echo "✅ Migration processor Lambda code deployed successfully"
            else
              echo "⚠️  Migration processor deployment may have issues, but continuing..."
            fi
          else
            echo "⚠️  Migration processor Lambda name not found"
          fi
        else
          echo "⚠️  Migration processor directory not found (checked: migration-processor, migrationprocessor, processor, backend/migration-processor, backend/processor)"
          echo "📁 Available directories:"
          ls -la . | grep ^d || echo "No directories found"
          echo "💡 Note: If your migration processor code is in backend/, consider creating a separate migration-processor/ directory"
        fi

    - name: Configure Lambda environment variables with race condition protection
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "⚙️  Configuring Lambda environment variables with race condition protection..."
        
        # Configure backend Lambda
        if [[ -n "$BACKEND_LAMBDA_NAME" && "$BACKEND_LAMBDA_NAME" != "null" ]]; then
          if wait_for_lambda_ready "$BACKEND_LAMBDA_NAME" "configuration"; then
            echo "🔧 Configuring backend Lambda environment variables..."
            aws lambda update-function-configuration \
              --function-name "$BACKEND_LAMBDA_NAME" \
              --environment "Variables={ENVIRONMENT=prod,SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE,AUDIT_TABLE=$AUDIT_TABLE,JOBS_TABLE=$JOBS_TABLE,LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT}"
            echo "✅ Backend Lambda environment variables configured"
          else
            echo "⚠️  Backend Lambda not ready for configuration, skipping..."
          fi
        else
          echo "⚠️  Backend Lambda name not found, skipping environment configuration"
        fi
        
        # Configure migration processor Lambda
        if [[ -n "$MIGRATION_LAMBDA_NAME" && "$MIGRATION_LAMBDA_NAME" != "null" ]]; then
          if wait_for_lambda_ready "$MIGRATION_LAMBDA_NAME" "configuration"; then
            echo "🔧 Configuring migration processor environment variables..."
            aws lambda update-function-configuration \
              --function-name "$MIGRATION_LAMBDA_NAME" \
              --environment "Variables={ENVIRONMENT=prod,UPLOAD_BUCKET=$UPLOAD_BUCKET,SUBSCRIBER_TABLE=$SUBSCRIBER_TABLE,JOBS_TABLE=$JOBS_TABLE,LEGACY_DB_ENDPOINT=$LEGACY_DB_ENDPOINT}"
            echo "✅ Migration processor environment variables configured"
          else
            echo "⚠️  Migration processor Lambda not ready for configuration, skipping..."
          fi
        else
          echo "⚠️  Migration processor name not found, skipping environment configuration"
        fi
        
        echo "✅ Lambda environment variable configuration completed"

    - name: Setup S3 notifications automatically  
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        echo "📡 Setting up S3 → Lambda notifications..."
        
        # Ensure migration processor Lambda is ready before setting up notifications
        if [[ -n "$MIGRATION_LAMBDA_NAME" && "$MIGRATION_LAMBDA_NAME" != "null" ]]; then
          # Add Lambda permission (allow failure if already exists)
          aws lambda add-permission \
            --function-name "$MIGRATION_LAMBDA_NAME" \
            --statement-id s3invoke \
            --action lambda:InvokeFunction \
            --principal s3.amazonaws.com \
            --source-arn "arn:aws:s3:::$UPLOAD_BUCKET" 2>/dev/null || echo "Permission already exists"
          
          # Configure S3 notifications (fail fast - no error suppression)
          aws s3api put-bucket-notification-configuration \
            --bucket "$UPLOAD_BUCKET" \
            --notification-configuration "{
              \"LambdaFunctionConfigurations\": [{
                \"Id\": \"MigrationProcessorTrigger\",
                \"LambdaFunctionArn\": \"$MIGRATION_ARN\",
                \"Events\": [\"s3:ObjectCreated:*\"],
                \"Filter\": {\"Key\": {\"FilterRules\": [{\"Name\": \"prefix\", \"Value\": \"uploads/\"}]}}
              }]
            }"
          
          echo "✅ S3 notifications configured"
        else
          echo "⚠️  Migration processor Lambda not available, skipping S3 notifications"
        fi

    - name: Deploy frontend automatically
      if: env.DEPLOYMENT_SUCCESS == 'true'  
      run: |
        echo "🌐 Deploying React frontend..."
        
        if [[ -d "frontend" ]]; then
          cd frontend
          
          # Install dependencies and build
          if [[ -f "package.json" ]]; then
            echo "📦 Installing frontend dependencies..."
            npm ci
            
            echo "🏗️  Building frontend..."
            npm run build
            
            # Upload to S3 (using cached environment variable)
            echo "📤 Uploading frontend to S3..."
            aws s3 sync build/ "s3://$FRONTEND_BUCKET" --delete
            
            echo "✅ Frontend deployed"
            echo "🌐 Frontend URL: $FRONTEND_URL"
          else
            echo "⚠️  package.json not found in frontend directory"
          fi
          
          cd ..
        else
          echo "⚠️  Frontend directory not found, skipping"
          echo "📁 Available directories:"
          ls -la . | grep ^d || echo "No directories found"
        fi

    - name: Initialize database schema with Docker
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        echo "🗄️  Initializing database schema using Docker..."
        
        # Get password from secrets manager
        SECRET_VALUE=$(aws secretsmanager get-secret-value --secret-id "$LEGACY_DB_SECRET" --query 'SecretString' --output text)
        DB_PASSWORD=$(echo "$SECRET_VALUE" | jq -r '.password')
        
        # Security: Mask the database password
        echo "::add-mask::$DB_PASSWORD"
        
        # Create SQL schema and execute using Docker (fast, reliable, secure)
        echo "🐳 Running database schema initialization..."
        cat << 'EOF' | docker run --rm -i -e MYSQL_PWD="$DB_PASSWORD" mysql:8.0 mysql -h "$LEGACY_DB_ENDPOINT" -u admin
        CREATE DATABASE IF NOT EXISTS legacydb;
        USE legacydb;
        
        CREATE TABLE IF NOT EXISTS subscribers (
          id VARCHAR(64) PRIMARY KEY,
          email VARCHAR(255) NOT NULL UNIQUE,
          name VARCHAR(255),
          phone VARCHAR(20),
          status VARCHAR(50) DEFAULT 'active',
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
          INDEX idx_email (email),
          INDEX idx_status (status)
        );
        
        CREATE TABLE IF NOT EXISTS migration_logs (
          id VARCHAR(64) PRIMARY KEY,
          job_id VARCHAR(64),
          file_name VARCHAR(255),
          total_records INT DEFAULT 0,
          processed_count INT DEFAULT 0,
          failed_count INT DEFAULT 0,
          status VARCHAR(50) DEFAULT 'processing',
          error_message TEXT,
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          completed_at TIMESTAMP NULL,
          INDEX idx_job_id (job_id),
          INDEX idx_status (status)
        );
        EOF
        
        echo "✅ Database schema initialized"

    - name: Run comprehensive smoke tests
      if: env.DEPLOYMENT_SUCCESS == 'true'
      run: |
        echo "🧪 Running comprehensive smoke tests..."
        
        # Test API health
        echo "🔍 Testing API health endpoint..."
        if curl -f -s "$BACKEND_API_URL/health" >/dev/null 2>&1; then
          echo "✅ API health check passed"
        else
          echo "⚠️  API health check failed (may be normal for new deployments)"
        fi
        
        # Test API CORS
        echo "🔍 Testing API CORS..."
        CORS_HEADERS=$(curl -s -I -X OPTIONS "$BACKEND_API_URL" 2>/dev/null | grep -i "access-control-allow" || echo "")
        if [[ -n "$CORS_HEADERS" ]]; then
          echo "✅ CORS headers present"
        else
          echo "⚠️  CORS headers not found"
        fi
        
        # Test frontend
        echo "🔍 Testing frontend availability..."
        if curl -f -s "$FRONTEND_URL" >/dev/null 2>&1; then
          echo "✅ Frontend accessible"
        else
          echo "⚠️  Frontend not accessible yet"
        fi
        
        # Test S3 upload capability
        echo "🔍 Testing S3 upload capability..."
        echo "test file content" > test-upload.txt
        if aws s3 cp test-upload.txt "s3://$UPLOAD_BUCKET/test/test-upload.txt" 2>/dev/null; then
          echo "✅ S3 upload test passed"
          aws s3 rm "s3://$UPLOAD_BUCKET/test/test-upload.txt" 2>/dev/null || true
        else
          echo "⚠️  S3 upload test failed"
        fi
        rm -f test-upload.txt
        
        echo ""
        echo "🎉 === FULL DEPLOYMENT COMPLETED SUCCESSFULLY ==="
        echo "🌐 Frontend: $FRONTEND_URL"
        echo "📡 API: $BACKEND_API_URL" 
        echo "📁 Upload Bucket: $UPLOAD_BUCKET"
        echo "✅ All services deployed and configured automatically!"
        echo ""
        echo "📋 Next Steps:"
        echo "1. Visit the frontend URL to see your application"
        echo "2. Test file uploads through the UI"
        echo "3. Check CloudWatch logs for any issues"
        echo "4. Monitor DynamoDB tables for processed data"
        echo ""
        echo "🔧 Debugging Commands (if needed):"
        echo "aws logs tail /aws/lambda/$BACKEND_LAMBDA_NAME --follow"
        echo "aws logs tail /aws/lambda/$MIGRATION_LAMBDA_NAME --follow"

    - name: Error reporting and cleanup
      if: failure()
      run: |
        # Source the functions
        source /tmp/stack_functions.sh
        
        echo "💥 === DEPLOYMENT WORKFLOW FAILED ==="
        
        STACK_NAME="${{ env.STACK_NAME }}"
        AWS_REGION="${{ env.AWS_REGION }}"
        
        # Generate error report
        echo ""
        echo "📋 === ERROR REPORT ==="
        echo "Timestamp: $(date -u)"
        echo "Stack Name: $STACK_NAME"
        echo "Region: $AWS_REGION"
        echo "Workflow: ${{ github.workflow }}"
        echo "Run ID: ${{ github.run_id }}"
        echo ""
        
        # Check stack status safely
        STACK_STATUS=$(get_stack_status "$STACK_NAME" "$AWS_REGION")
        echo "📊 Current Stack Status: $STACK_STATUS"
        
        if [[ "$STACK_STATUS" != "STACK_NOT_EXISTS" ]]; then
          echo ""
          echo "📋 Recent Stack Events:"
          aws cloudformation describe-stack-events --stack-name "$STACK_NAME" --region "$AWS_REGION" \
            --query 'StackEvents[0:20].[Timestamp,LogicalResourceId,ResourceType,ResourceStatus,ResourceStatusReason]' \
            --output table 2>/dev/null || echo "Could not retrieve stack events"
          
          # Cleanup failed stack
          if [[ "$STACK_STATUS" == *"_FAILED" || "$STACK_STATUS" == *"_ROLLBACK_COMPLETE" ]]; then
            echo ""
            echo "🧹 Automated cleanup..."
            delete_failed_stack_robust "$STACK_NAME" "$AWS_REGION" || true
            
            # Clean up S3 buckets
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            delete_s3_bucket_enhanced "sub-mig-logs-$ACCOUNT_ID" || true
            delete_s3_bucket_enhanced "sub-mig-web-$ACCOUNT_ID-prod" || true
            delete_s3_bucket_enhanced "sub-mig-data-$ACCOUNT_ID-prod" || true
          fi
        else
          echo "ℹ️  Stack does not exist - no cleanup needed"
        fi
        
        echo ""
        echo "💾 Error report completed"
        exit 1
