name: ðŸš€ Prod-Only Deployment with Step Functions Orchestration

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_DEFAULT_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-prod
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  validate:
    name: ðŸ” Validate Architecture
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install validation tools
        run: pip install aws-sam-cli boto3 jq
      - name: Validate SAM Template
        run: |
          set -euo pipefail
          cd aws
          sam validate --template template.yaml --region ${{ env.AWS_DEFAULT_REGION }}

  discover-infrastructure:
    name: ðŸ” Discover Existing VPC and Subnets
    runs-on: ubuntu-latest
    needs: validate
    outputs:
      vpc-id: ${{ steps.discover.outputs.vpc_id }}
      private-subnet-1: ${{ steps.discover.outputs.private_subnet_1 }}
      private-subnet-2: ${{ steps.discover.outputs.private_subnet_2 }}
      public-subnet-1: ${{ steps.discover.outputs.public_subnet_1 }}
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: ðŸ” Auto-discover VPC and Subnets
        id: discover
        run: |
          set -euo pipefail
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=is-default,Values=true" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo None)
          if [[ "$VPC_ID" == "None" ]]; then
            VPC_ID=$(aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo None)
          fi
          if [[ "$VPC_ID" == "None" ]]; then
            echo "âŒ No VPC found"; exit 1
          fi
          PRIVATE_SUBNETS=($(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[?!MapPublicIpOnLaunch].SubnetId' --output text || echo ""))
          PRIVATE_1=${PRIVATE_SUBNETS[0]:-""}
          PRIVATE_2=${PRIVATE_SUBNETS[1]:-""}
          if [[ -z "$PRIVATE_1" || -z "$PRIVATE_2" ]]; then
            echo "âŒ Need 2 private subnets"; exit 1
          fi
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "private_subnet_1=$PRIVATE_1" >> $GITHUB_OUTPUT
          echo "private_subnet_2=$PRIVATE_2" >> $GITHUB_OUTPUT
          echo "public_subnet_1=" >> $GITHUB_OUTPUT

  deploy:
    name: ðŸ—ï¸ Deploy Infrastructure with Existing VPC
    runs-on: ubuntu-latest
    needs: [validate, discover-infrastructure]
    environment: production
    outputs:
      api-endpoint: ${{ steps.out.outputs.api_endpoint }}
      schema-function-name: ${{ steps.out.outputs.schema_function_name }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: ðŸ§¹ Cleanup blocked stack and orphaned resources (if any)
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          REGION="${{ env.AWS_DEFAULT_REGION }}"
          status=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo NONE)
          echo "Status: $status"
          is_blocked(){ case "$1" in ROLLBACK_COMPLETE|ROLLBACK_FAILED|DELETE_FAILED|UPDATE_ROLLBACK_COMPLETE|UPDATE_ROLLBACK_FAILED) return 0;; *) return 1;; esac }
          if is_blocked "$status"; then
            echo "ðŸ§¹ Deleting blocked stack $STACK"
            # Empty S3 buckets that may block deletion
            for b in $(aws s3api list-buckets --query 'Buckets[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}|frontend|uploads" || true); do
              aws s3 rm "s3://$b" --recursive || true
              aws s3 rb "s3://$b" --force || true
            done
            # Delete stack and wait
            aws cloudformation delete-stack --stack-name "$STACK" || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            echo "âœ… Blocked stack cleaned up"
          else
            echo "âœ… No blocked stack"
          fi
          echo "ðŸ§¹ Cleaning orphans (best-effort)"
          # Step Functions orphaned state machines
          for sf in $(aws stepfunctions list-state-machines --query 'stateMachines[].stateMachineArn' --output text || true); do
            name=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query name --output text || echo "")
            if echo "$name" | grep -q "${STACK}"; then aws stepfunctions delete-state-machine --state-machine-arn "$sf" || true; fi
          done
          # Lambda functions
          for f in $(aws lambda list-functions --query 'Functions[].FunctionName' --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws lambda delete-function --function-name "$f" || true; done
          # API Gateway REST APIs
          for id in $(aws apigateway get-rest-apis --query 'items[].{id:id,name:name}' --output text 2>/dev/null | awk '{print $1" "$2}' | grep "${STACK}" | awk '{print $1}' || true); do aws apigateway delete-rest-api --rest-api-id "$id" || true; done
          # DynamoDB tables
          for t in $(aws dynamodb list-tables --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws dynamodb delete-table --table-name "$t" || true; done
          # Secrets
          for s in $(aws secretsmanager list-secrets --query 'SecretList[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}-legacy-db|${STACK}-users" || true); do aws secretsmanager delete-secret --secret-id "$s" --force-delete-without-recovery || true; done
          # CloudWatch Logs
          for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/lambda/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
          for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/stepfunctions/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done

      - name: Install tooling
        run: pip install aws-sam-cli boto3 jq

      - name: ðŸš€ Build & Deploy SAM with Existing VPC (Dynamic Parameters)
        id: deploy_step
        run: |
          set -euo pipefail
          cd aws
          sam build --template template.yaml
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="sam-deployment-${ACCOUNT_ID}-${{ env.AWS_DEFAULT_REGION }}"
          aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$BUCKET"
          JWT_SECRET=$(openssl rand -base64 48)
          VPC_ID="${{ needs.discover-infrastructure.outputs.vpc-id }}"
          PRIVATE_1="${{ needs.discover-infrastructure.outputs.private-subnet-1 }}"
          PRIVATE_2="${{ needs.discover-infrastructure.outputs.private-subnet-2 }}"
          PUBLIC_1="${{ needs.discover-infrastructure.outputs.public-subnet-1 }}"
          echo "ðŸ”§ Using discovered infrastructure:\n  VPC: $VPC_ID\n  Private: $PRIVATE_1, $PRIVATE_2\n  Public: ${PUBLIC_1:-<none>}"
          PARAMS=(
            "Stage=prod"
            "JwtSecret=$JWT_SECRET"
            "CorsOrigins=https://yourdomain.com"
            "VpcId=$VPC_ID"
            "PrivateSubnetId1=$PRIVATE_1"
            "PrivateSubnetId2=$PRIVATE_2"
          )
          [[ -n "$PUBLIC_1" ]] && PARAMS+=("PublicSubnetId1=$PUBLIC_1")
          echo "ðŸ“¦ Parameter count: ${#PARAMS[@]}"; echo "ðŸ“ Parameters: ${PARAMS[*]}"
          sam deploy \
            --stack-name "${{ env.STACK_NAME }}" \
            --region ${{ env.AWS_DEFAULT_REGION }} \
            --s3-bucket "$BUCKET" \
            --parameter-overrides "${PARAMS[@]}" \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --no-fail-on-empty-changeset \
            --no-confirm-changeset

      - name: ðŸ“¤ Export outputs
        id: out
        run: |
          set -euo pipefail
          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' --output text)
          SCHEMA_FUNCTION_NAME=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`SchemaInitializerFunctionName`].OutputValue' --output text)
          echo "api_endpoint=$API_ENDPOINT" >> $GITHUB_OUTPUT
          echo "schema_function_name=$SCHEMA_FUNCTION_NAME" >> $GITHUB_OUTPUT

  initialize-legacy-schema:
    name: ðŸ—ƒï¸ Initialize Legacy DB Schema via VPC Lambda
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: ðŸ“¦ Install tooling
        run: pip install boto3
      - name: ðŸ—ƒï¸ Parse SQL file
        run: |
          set -euo pipefail
          if [ ! -f database/rds_schema_update.sql ]; then echo '[]' > sql_statements.json; else python3 - <<'PY'
import json
sql=open('database/rds_schema_update.sql','r',encoding='utf-8').read()
stmts=[s.strip() for s in sql.split(';') if s.strip() and not s.strip().startswith('--')]
open('sql_statements.json','w').write(json.dumps(stmts))
print(f"âœ… Parsed {len(stmts)} SQL statements")
PY
          fi
      - name: ðŸš€ Invoke Schema Initializer Lambda
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          FN=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='SchemaInitializerFunctionName'].OutputValue" --output text)
          if [[ -z "$FN" || "$FN" == "None" ]]; then echo "âŒ Schema initializer not found"; aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].Outputs[].OutputKey' --output text; exit 1; fi
          SQL=$(cat sql_statements.json)
          PAYLOAD=$(jq -n --argjson stmts "$SQL" '{sql_statements:$stmts}')
          aws lambda invoke --function-name "$FN" --payload "$PAYLOAD" --cli-binary-format raw-in-base64-out --log-type Tail response.json
          echo "ðŸ“¥ Response:"; cat response.json | jq '.' || cat response.json
          BODY=$(jq -r '.body' response.json)
          [[ "$BODY" == "null" ]] && echo "âŒ No body" && exit 1
          SUCCESS=$(echo "$BODY" | jq -r '.success // false')
          ERRORS=$(echo "$BODY" | jq -r '.summary.errors // 0')
          EXECUTED=$(echo "$BODY" | jq -r '.summary.executed // 0')
          SKIPPED=$(echo "$BODY" | jq -r '.summary.skipped // 0')
          AUTO_CREATED=$(echo "$BODY" | jq -r '.summary.auto_created_tables // 0')
          echo "ðŸ“Š Executed:$EXECUTED Skipped:$SKIPPED AutoCreated:$AUTO_CREATED Errors:$ERRORS"
          if [[ "$SUCCESS" != "true" && "$ERRORS" -gt 10 && "$EXECUTED" -lt 3 ]]; then echo "âŒ Critical schema issues"; exit 1; fi

  frontend-deploy:
    name: ðŸŒ Frontend Deploy
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema]
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: ðŸ“¦ Install & Build Frontend
        run: |
          cd frontend
          if [[ -f package-lock.json ]]; then npm ci || { rm -rf node_modules package-lock.json; npm install; }; else npm install; fi
          [[ ! -f package-lock.json ]] && npm install --package-lock-only
          echo "REACT_APP_API_URL=${{ needs.deploy.outputs.api-endpoint }}" > .env.production
          echo "REACT_APP_STAGE=prod" >> .env.production
          npm run build
      - name: ðŸš€ Upload to S3 website
        run: |
          FRONTEND_BUCKET="${{ env.STACK_NAME }}-frontend"
          aws s3api head-bucket --bucket "$FRONTEND_BUCKET" 2>/dev/null || {
            if [[ "${{ env.AWS_DEFAULT_REGION }}" == "us-east-1" ]]; then aws s3api create-bucket --bucket "$FRONTEND_BUCKET"; else aws s3api create-bucket --bucket "$FRONTEND_BUCKET" --create-bucket-configuration LocationConstraint="${{ env.AWS_DEFAULT_REGION }}"; fi
          }
          aws s3 sync frontend/build/ s3://$FRONTEND_BUCKET --delete
          aws s3api put-bucket-website --bucket "$FRONTEND_BUCKET" --website-configuration '{"IndexDocument":{"Suffix":"index.html"},"ErrorDocument":{"Key":"index.html"}}'
          aws s3api put-bucket-policy --bucket "$FRONTEND_BUCKET" --policy '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":"*","Action":"s3:GetObject","Resource":"arn:aws:s3:::'$FRONTEND_BUCKET'/*"}]}'

  notify-completion:
    name: ðŸ“§ Deployment Complete
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy]
    if: always()
    steps:
      - name: ðŸŽ‰ Deployment Summary
        run: |
          echo "ðŸŽ† Deployment Complete";
          echo "Stack: ${{ env.STACK_NAME }}";
          echo "API: ${{ needs.deploy.outputs.api-endpoint }}";
          echo "Schema Function: ${{ needs.deploy.outputs.schema-function-name }}";
          echo "Timestamp: $(date)";
