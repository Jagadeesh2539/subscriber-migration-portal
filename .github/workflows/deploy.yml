name: üöÄ Prod-Only Deployment with Step Functions Orchestration

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_DEFAULT_REGION: us-east-1
  STACK_NAME: subscriber-migration-portal-prod
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  validate:
    name: üîç Validate Architecture
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install validation tools
        run: pip install aws-sam-cli boto3 jq
      - name: Validate SAM Template
        run: |
          set -euo pipefail
          cd aws
          sam validate --template template.yaml --region ${{ env.AWS_DEFAULT_REGION }}
      - name: Validate Step Functions Definitions
        run: |
          set -euo pipefail
          echo "üîç Validating Step Functions JSON syntax..."
          for file in aws/stepfunctions/*.json; do
            if [[ -f "$file" ]]; then
              filename=$(basename "$file")
              echo "Validating: $filename"
              
              # Validate JSON syntax
              if ! jq empty "$file" 2>/dev/null; then
                echo "‚ùå Invalid JSON syntax in $file"
                exit 1
              fi
              
              # Check required fields
              if ! jq -e '.StartAt' "$file" >/dev/null; then
                echo "‚ùå Missing StartAt field in $file"
                exit 1
              fi
              
              if ! jq -e '.States' "$file" >/dev/null; then
                echo "‚ùå Missing States field in $file"
                exit 1
              fi
              
              echo "‚úÖ Valid: $filename"
            fi
          done
          echo "üéâ All Step Functions definitions are valid!"

  deploy-stepfunctions:
    name: üì¶ Deploy Step Functions Definitions
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: Setup S3 bucket for Step Functions
        run: |
          set -euo pipefail
          UPLOADS_BUCKET="${{ env.STACK_NAME }}-uploads"
          
          # Create bucket if it doesn't exist
          if ! aws s3api head-bucket --bucket "$UPLOADS_BUCKET" 2>/dev/null; then
            echo "üöÄ Creating S3 bucket: $UPLOADS_BUCKET"
            if [[ "${{ env.AWS_DEFAULT_REGION }}" == "us-east-1" ]]; then
              aws s3api create-bucket --bucket "$UPLOADS_BUCKET"
            else
              aws s3api create-bucket \
                --bucket "$UPLOADS_BUCKET" \
                --create-bucket-configuration LocationConstraint="${{ env.AWS_DEFAULT_REGION }}"
            fi
            
            # Enable versioning and security
            aws s3api put-bucket-versioning \
              --bucket "$UPLOADS_BUCKET" \
              --versioning-configuration Status=Enabled
            
            aws s3api put-public-access-block \
              --bucket "$UPLOADS_BUCKET" \
              --public-access-block-configuration \
              "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            
            echo "‚úÖ S3 bucket created and secured: $UPLOADS_BUCKET"
          else
            echo "‚úÖ S3 bucket already exists: $UPLOADS_BUCKET"
          fi
      - name: Upload Step Functions definitions
        run: |
          set -euo pipefail
          UPLOADS_BUCKET="${{ env.STACK_NAME }}-uploads"
          
          echo "üì¶ Uploading Step Functions definitions to S3..."
          
          # Create stepfunctions directory in S3
          aws s3api put-object \
            --bucket "$UPLOADS_BUCKET" \
            --key "stepfunctions/" \
            --content-length 0
          
          # Upload each Step Function definition
          for file in aws/stepfunctions/*.json; do
            if [[ -f "$file" ]]; then
              filename=$(basename "$file")
              s3_key="stepfunctions/$filename"
              
              echo "üì§ Uploading $filename to s3://$UPLOADS_BUCKET/$s3_key"
              
              aws s3 cp "$file" "s3://$UPLOADS_BUCKET/$s3_key" \
                --content-type "application/json"
              
              echo "‚úÖ Uploaded: $filename"
            fi
          done
          
          echo "üéâ All Step Functions definitions uploaded successfully!"

  deploy:
    name: üèóÔ∏è Deploy Infrastructure with Step Functions
    runs-on: ubuntu-latest
    needs: [validate, deploy-stepfunctions]
    environment: production
    outputs:
      api-endpoint: ${{ steps.out.outputs.api_endpoint }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install tooling
        run: pip install aws-sam-cli boto3 jq

      - name: üî• Detect and cleanup failed/rollback stacks BEFORE deploy
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "NONE")
          echo "Status: $STATUS"
          is_bad() { case "$1" in ROLLBACK_*|*_ROLLBACK_*|*_FAILED|DELETE_FAILED) return 0;; *) return 1;; esac }
          cleanup() {
            echo "üßπ Cleaning resources for $STACK"
            # S3 buckets
            for b in $(aws s3api list-buckets --query 'Buckets[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}|frontend|uploads" || true); do
              aws s3 rm "s3://$b" --recursive || true; aws s3 rb "s3://$b" --force || true; done
            # Step Functions
            for sf in $(aws stepfunctions list-state-machines --query 'stateMachines[?contains(name, `'$STACK'`)].stateMachineArn' --output text || true); do
              aws stepfunctions delete-state-machine --state-machine-arn "$sf" || true; done
            # Lambda
            for f in $(aws lambda list-functions --query 'Functions[].FunctionName' --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws lambda delete-function --function-name "$f" || true; done
            # API Gateway
            for id in $(aws apigateway get-rest-apis --query 'items[?contains(name, `'$STACK'`)].id' --output text || true); do aws apigateway delete-rest-api --rest-api-id "$id" || true; done
            # DynamoDB by tag
            for t in $(aws dynamodb list-tables --output text | tr '\t' '\n' | grep -E "${STACK}" || true); do aws dynamodb delete-table --table-name "$t" || true; done
            # RDS instances
            for db in $(aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `'$STACK'`)].DBInstanceIdentifier' --output text || true); do aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot || true; done
            # Logs
            for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/lambda/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
            for g in $(aws logs describe-log-groups --log-group-name-prefix "/aws/stepfunctions/${STACK}" --query 'logGroups[].logGroupName' --output text || true); do aws logs delete-log-group --log-group-name "$g" || true; done
            # Secrets
            for s in $(aws secretsmanager list-secrets --query 'SecretList[].Name' --output text | tr '\t' '\n' | grep -E "${STACK}-legacy-db|${STACK}-users" || true); do aws secretsmanager delete-secret --secret-id "$s" --force-delete-without-recovery || true; done
          }
          if is_bad "$STATUS"; then
            echo "üóëÔ∏è Deleting bad stack $STACK"; aws cloudformation delete-stack --stack-name "$STACK" || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            cleanup
          fi

      - name: üöÄ Build & Deploy SAM with Step Functions
        id: deploy_step
        run: |
          set -euo pipefail
          cd aws
          sam build --template template.yaml
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="sam-deployment-${ACCOUNT_ID}-${{ env.AWS_DEFAULT_REGION }}"
          aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$BUCKET"
          JWT_SECRET=$(openssl rand -base64 48)
          sam deploy \
            --stack-name "${{ env.STACK_NAME }}" \
            --region ${{ env.AWS_DEFAULT_REGION }} \
            --s3-bucket "$BUCKET" \
            --parameter-overrides Stage=prod JwtSecret="$JWT_SECRET" CorsOrigins="https://yourdomain.com" \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --no-fail-on-empty-changeset \
            --no-confirm-changeset

      - name: üì§ Export API endpoint
        id: out
        run: |
          set -euo pipefail
          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "${{ env.STACK_NAME }}" --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' --output text)
          echo "api_endpoint=$API_ENDPOINT" >> $GITHUB_OUTPUT

  initialize-legacy-schema:
    name: üóÑÔ∏è Initialize Legacy DB Schema
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üì¶ Install DB tooling
        run: pip install boto3 pymysql
      - name: üîç Fetch legacy DB outputs
        id: cf
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          LEGACY_DB_SECRET_ARN=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='LegacyDbSecretArn'].OutputValue" --output text)
          LEGACY_DB_HOST=$(aws cloudformation describe-stacks --stack-name "$STACK" --query "Stacks[0].Outputs[?OutputKey=='LegacyDbHost'].OutputValue" --output text)
          echo "legacy_db_secret_arn=$LEGACY_DB_SECRET_ARN" >> $GITHUB_OUTPUT
          echo "legacy_db_host=$LEGACY_DB_HOST" >> $GITHUB_OUTPUT
      - name: üóÑÔ∏è Run schema initializer
        env:
          LEGACY_DB_SECRET_ARN: ${{ steps.cf.outputs.legacy_db_secret_arn }}
          LEGACY_DB_HOST: ${{ steps.cf.outputs.legacy_db_host }}
        run: |
          set -euo pipefail
          if [ -f database/rds_schema_update.sql ]; then
            echo "üîç Found schema file: database/rds_schema_update.sql"
            python - <<'PY'
import os, json, boto3, pymysql, sys
region = os.environ.get("AWS_REGION") or os.environ.get("AWS_DEFAULT_REGION") or "us-east-1"
secret_arn = os.environ["LEGACY_DB_SECRET_ARN"]
host = os.environ["LEGACY_DB_HOST"]
sql_file = "database/rds_schema_update.sql"
sm = boto3.client("secretsmanager", region_name=region)
sec = json.loads(sm.get_secret_value(SecretId=secret_arn)["SecretString"])
user = sec.get("username") or sec.get("user")
pwd  = sec.get("password") or sec.get("pass")
db   = sec.get("dbname") or sec.get("database") or ""
if not (user and pwd):
    print("‚ùå Secret missing username/password", file=sys.stderr)
    sys.exit(1)
conn = pymysql.connect(host=host, user=user, password=pwd, database=db, autocommit=True, connect_timeout=30, charset="utf8mb4")
executed = skipped = errors = 0
with conn.cursor() as cur, open(sql_file, "r", encoding="utf-8") as f:
    sql = f.read()
    stmts = [s.strip() for s in sql.split(";") if s.strip() and not s.strip().startswith("--")]
    for stmt in stmts:
        try:
            cur.execute(stmt)
            executed += 1
            print(f"Executed: {stmt[:80]}...")
        except Exception as e:
            msg = str(e).lower()
            if "already exists" in msg or "duplicate" in msg or "exists" in msg:
                skipped += 1
                print(f"Skip exists: {stmt[:80]}...")
            else:
                errors += 1
                print(f"Error executing {stmt[:80]}...: {e}", file=sys.stderr)
print(f"‚úÖ Legacy schema initialization completed: executed={executed}, skipped={skipped}, errors={errors}")
if errors:
    sys.exit(1)
PY
          else
            echo "‚ùå ERROR: Schema file NOT FOUND at database/rds_schema_update.sql"; exit 1
          fi

  frontend-deploy:
    name: üåê Frontend Deploy
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema]
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: üì¶ Install & Build
        run: |
          cd frontend
          npm ci || (npm cache verify && rm -rf node_modules package-lock.json && npm install --force)
          echo "REACT_APP_API_URL=${{ needs.deploy.outputs.api-endpoint }}" > .env.production
          echo "REACT_APP_STAGE=prod" >> .env.production
          npm run build
      - name: üöÄ Upload to S3 website
        run: |
          FRONTEND_BUCKET="${{ env.STACK_NAME }}-frontend"
          aws s3api head-bucket --bucket "$FRONTEND_BUCKET" 2>/dev/null || aws s3api create-bucket --bucket "$FRONTEND_BUCKET"
          aws s3 sync frontend/build/ s3://$FRONTEND_BUCKET --delete
          aws s3api put-bucket-website --bucket "$FRONTEND_BUCKET" --website-configuration '{
            "IndexDocument": {"Suffix": "index.html"},
            "ErrorDocument": {"Key": "index.html"}
          }'

  comprehensive-smoke-tests:
    name: üß™ Comprehensive Smoke Tests with Step Functions Validation
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: üì¶ Install test dependencies
        run: pip install requests pytest boto3
      - name: üß™ Run comprehensive smoke tests
        env:
          API_ENDPOINT: ${{ needs.deploy.outputs.api-endpoint }}
          STACK_NAME: ${{ env.STACK_NAME }}
        run: |
          set -euo pipefail
          cat > comprehensive_smoke_tests.py << 'PYEOF'
import requests, time, sys, os, json, uuid
from datetime import datetime
import boto3
class ComprehensiveSmokeTests:
    def __init__(self, base_url, stack_name):
        self.base_url = base_url.rstrip('/')
        self.stack_name = stack_name
        self.token = None
        self.results = []
        try:
            self.sfn_client = boto3.client('stepfunctions')
            self.cf_client = boto3.client('cloudformation')
        except Exception:
            self.sfn_client = None
            self.cf_client = None
    def log(self, name, ok, msg, dur):
        print(("‚úÖ" if ok else "‚ùå"), name+":", msg, f"({dur:.2f}s)")
        self.results.append(ok)
    def health(self):
        t=time.time()
        try:
            r=requests.get(f"{self.base_url}/health", timeout=10)
            self.log("API Health", r.status_code==200, f"HTTP {r.status_code}", time.time()-t)
            return r.status_code==200
        except Exception as e:
            self.log("API Health", False, str(e), time.time()-t); return False
    def auth(self):
        t=time.time()
        try:
            r=requests.post(f"{self.base_url}/auth/login", json={"username":"admin","password":"SecureAdmin123!"}, timeout=10)
            ok=r.status_code==200 and r.json().get('data',{}).get('token')
            if ok: self.token=r.json()['data']['token']
            self.log("Authentication", bool(ok), f"HTTP {r.status_code}", time.time()-t)
            return bool(ok)
        except Exception as e:
            self.log("Authentication", False, str(e), time.time()-t); return False
    def stepfuncs(self):
        if not self.cf_client: self.log("Step Functions", False, "No CF client", 0); return False
        t=time.time()
        try:
            outs={o['OutputKey']:o['OutputValue'] for o in self.cf_client.describe_stacks(StackName=self.stack_name)['Stacks'][0].get('Outputs',[])}
            keys=['MigrationWorkflowArn','AuditWorkflowArn','ExportWorkflowArn']
            ok=all(k in outs for k in keys)
            if self.sfn_client:
                for k in keys:
                    if k in outs:
                        try:
                            d=self.sfn_client.describe_state_machine(stateMachineArn=outs[k]); print("  ",k, d.get('status','UNKNOWN'))
                        except Exception as e:
                            print("  ",k, "ERR", e)
            self.log("Step Functions", ok, f"Have {sum(1 for k in keys if k in outs)}/3", time.time()-t)
            return ok
        except Exception as e:
            self.log("Step Functions", False, str(e), time.time()-t); return False
    def settings(self):
        if not self.token: self.log("Settings", False, "No token", 0); return False
        t=time.time(); h={"Authorization":f"Bearer {self.token}"}
        try:
            r=requests.get(f"{self.base_url}/settings/provisioning-mode", headers=h, timeout=10)
            ok=r.status_code==200 and (r.json().get('data',{}).get('mode') or r.json().get('mode')) in ['CLOUD','LEGACY','DUAL_PROV']
            self.log("Settings", ok, f"HTTP {r.status_code}", time.time()-t)
            return ok
        except Exception as e:
            self.log("Settings", False, str(e), time.time()-t); return False
    def jobs(self):
        if not self.token: self.log("Jobs", False, "No token", 0); return False
        h={"Authorization":f"Bearer {self.token}"}; t=time.time()
        try:
            u=requests.post(f"{self.base_url}/migration/upload", json={"fileName":"test.csv","fileType":"text/csv"}, headers=h, timeout=15)
            ok=u.status_code==200 and u.json().get('data',{}).get('uploadUrl');
            if not ok: self.log("Jobs Upload", False, f"HTTP {u.status_code}", time.time()-t); return False
            t=time.time(); j=requests.post(f"{self.base_url}/audit/jobs", json={"jobType":"AUDIT","auditType":"CONSISTENCY_CHECK","filters":{"status":"ACTIVE"}}, headers=h, timeout=15)
            if j.status_code!=201 or not j.json().get('data',{}).get('jobId'):
                self.log("Jobs Create", False, f"HTTP {j.status_code}", time.time()-t); return False
            job_id=j.json()['data']['jobId']; self.log("Jobs Create", True, job_id, time.time()-t)
            time.sleep(2); t=time.time(); s=requests.get(f"{self.base_url}/jobs/{job_id}", headers=h, timeout=10)
            self.log("Jobs Status", s.status_code==200, f"HTTP {s.status_code}", time.time()-t)
            return s.status_code==200
        except Exception as e:
            self.log("Jobs", False, str(e), time.time()-t); return False
    def run(self):
        print("üß™ Testing", self.base_url, "at", datetime.now())
        for _ in range(24):
            try:
                if requests.get(f"{self.base_url}/health", timeout=5).status_code==200: break
            except Exception: pass
            time.sleep(10)
        tests=[self.health(), self.auth(), self.stepfuncs(), self.settings(), self.jobs()]
        ok=sum(tests); total=len(tests)
        print(f"üìä Results: {ok}/{total}")
        return ok>=total*0.8
if __name__=="__main__":
    base=os.environ.get('API_ENDPOINT'); stack=os.environ.get('STACK_NAME')
    if not base: print("‚ùå API_ENDPOINT not set"); sys.exit(1)
    sys.exit(0 if ComprehensiveSmokeTests(base, stack).run() else 1)
PYEOF
          python comprehensive_smoke_tests.py

  post-deployment-checks:
    name: üîç Post-Deployment System Validation
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy, comprehensive-smoke-tests]
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: üîç Validate infrastructure state
        run: |
          set -euo pipefail
          STACK="${{ env.STACK_NAME }}"
          echo "üîç Running post-deployment infrastructure validation..."
          
          # Check stack status
          STACK_STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" --query 'Stacks[0].StackStatus' --output text)
          if [[ "$STACK_STATUS" != "CREATE_COMPLETE" && "$STACK_STATUS" != "UPDATE_COMPLETE" ]]; then
            echo "‚ùå Stack not in healthy state: $STACK_STATUS"
            exit 1
          fi
          echo "‚úÖ CloudFormation stack healthy: $STACK_STATUS"
          
          # Check Step Functions
          for sf in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::StepFunctions::StateMachine`].PhysicalResourceId' --output text); do
            STATUS=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query 'status' --output text)
            if [[ "$STATUS" != "ACTIVE" ]]; then
              echo "‚ùå Step Function $sf not active: $STATUS"
              exit 1
            fi
            NAME=$(aws stepfunctions describe-state-machine --state-machine-arn "$sf" --query 'name' --output text)
            echo "‚úÖ Step Function $NAME is ACTIVE"
          done
          
          # Check DynamoDB tables
          for table in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::DynamoDB::Table`].PhysicalResourceId' --output text); do
            STATUS=$(aws dynamodb describe-table --table-name "$table" --query 'Table.TableStatus' --output text)
            if [[ "$STATUS" != "ACTIVE" ]]; then
              echo "‚ùå DynamoDB table $table not active: $STATUS"
              exit 1
            fi
            echo "‚úÖ DynamoDB table $table is ACTIVE"
          done
          
          # Check RDS instance
          RDS_ID=$(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::RDS::DBInstance`].PhysicalResourceId' --output text)
          if [[ -n "$RDS_ID" ]]; then
            RDS_STATUS=$(aws rds describe-db-instances --db-instance-identifier "$RDS_ID" --query 'DBInstances[0].DBInstanceStatus' --output text)
            if [[ "$RDS_STATUS" != "available" ]]; then
              echo "‚ùå RDS instance $RDS_ID not available: $RDS_STATUS"
              exit 1
            fi
            echo "‚úÖ RDS instance $RDS_ID is available"
          fi
          
          # Check Lambda functions
          for func in $(aws cloudformation describe-stack-resources --stack-name "$STACK" --query 'StackResources[?ResourceType==`AWS::Lambda::Function`].PhysicalResourceId' --output text); do
            STATE=$(aws lambda get-function --function-name "$func" --query 'Configuration.State' --output text)
            if [[ "$STATE" != "Active" ]]; then
              echo "‚ùå Lambda function $func not active: $STATE"
              exit 1
            fi
            echo "‚úÖ Lambda function $func is Active"
          done
          
          echo "üéâ All infrastructure components validated successfully!"

  notify-completion:
    name: üìß Notify Deployment Completion
    runs-on: ubuntu-latest
    needs: [deploy, initialize-legacy-schema, frontend-deploy, comprehensive-smoke-tests, post-deployment-checks]
    if: always()
    steps:
      - name: üéâ Deployment Summary
        run: |
          set -euo pipefail
          echo "üéÜ Deployment Summary"
          echo "==================="
          echo "Stack: ${{ env.STACK_NAME }}"
          echo "API Endpoint: ${{ needs.deploy.outputs.api-endpoint }}"
          echo "Deployment Status: ${{ needs.post-deployment-checks.result }}"
          echo "Timestamp: $(date)"
          echo ""
          
          if [[ "${{ needs.post-deployment-checks.result }}" == "success" ]]; then
            echo "‚úÖ Production deployment completed successfully!"
            echo "üéâ Your application is live and fully operational!"
          else
            echo "‚ùå Deployment completed with issues - check logs above"
          fi

permissions:
  contents: read
  id-token: write