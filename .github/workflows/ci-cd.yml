name: Deploy Subscriber Migration Portal

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      stage:
        description: 'Deployment stage'
        required: true
        default: 'prod'
        type: choice
        options: [dev, test, prod]

env:
  AWS_DEFAULT_REGION: us-west-2
  AWS_REGION: us-west-2
  STACK_NAME: subscriber-migration-portal-prod
  BUCKET_SUFFIX: '20251031'
  STAGE: prod

jobs:
  build-test-deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup SAM CLI
        uses: aws-actions/setup-sam@v2
        with:
          use-installer: true

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 pymysql

      - name: Build SAM application
        working-directory: aws
        run: sam build --use-container

      - name: Deploy with DB instance-class fallback (t3.micro ‚Üí t2.micro)
        id: deploy
        working-directory: aws
        env:
          DB_CLASSES: "db.t3.micro db.t2.micro"
        run: |
          set -euo pipefail
          STACK="$STACK_NAME"
          if [ "${{ inputs.stage || '' }}" != "" ] && [ "${{ inputs.stage }}" != "prod" ]; then
            STACK="subscriber-migration-portal-${{ inputs.stage }}"
          fi
          for DBC in $DB_CLASSES; do
            echo "Attempting deploy with DbInstanceClass=$DBC in $AWS_REGION"
            if sam deploy \
              --stack-name "$STACK" \
              --region "$AWS_REGION" \
              --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
              --no-confirm-changeset \
              --no-fail-on-empty-changeset \
              --parameter-overrides \
                Stage="${{ inputs.stage || env.STAGE }}" \
                JwtSecret="${{ secrets.JWT_SECRET }}" \
                CorsOrigins="${{ secrets.CORS_ORIGINS || 'https://yourdomain.com' }}" \
                BucketSuffix="$BUCKET_SUFFIX" \
                DbInstanceClass="$DBC"; then
              echo "db_class=$DBC" >> $GITHUB_OUTPUT
              echo "‚úÖ Deploy succeeded with $DBC"
              exit 0
            else
              echo "‚ö†Ô∏è Deploy failed with $DBC, trying next..."
              if aws cloudformation describe-stacks --region "$AWS_REGION" --stack-name "$STACK" >/dev/null 2>&1; then
                STATUS=$(aws cloudformation describe-stacks --region "$AWS_REGION" --stack-name "$STACK" --query "Stacks[0].StackStatus" --output text || true)
                echo "Stack status: $STATUS"
                if [[ "$STATUS" == *"ROLLBACK"* || "$STATUS" == *"FAILED"* ]]; then
                  echo "üóëÔ∏è Deleting bad stack $STACK"
                  aws cloudformation delete-stack --region "$AWS_REGION" --stack-name "$STACK"
                  aws cloudformation wait stack-delete-complete --region "$AWS_REGION" --stack-name "$STACK" || true
                  UPB="${STACK}-uploads-${BUCKET_SUFFIX}"
                  echo "üßπ Force-empty S3 bucket: $UPB in $AWS_REGION"
                  # delete object versions
                  while true; do
                    VERS=$(aws s3api list-object-versions --bucket "$UPB" --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' --output json --region "$AWS_REGION" || echo '{"Objects":[]}')
                    if [ "$(echo "$VERS" | python - <<'PY'
import sys, json; print(len(json.load(sys.stdin)["Objects"]))
PY
)" = "0" ]; then break; fi
                    aws s3api delete-objects --bucket "$UPB" --delete "$VERS" --region "$AWS_REGION" || true
                  done
                  # delete delete-markers
                  while true; do
                    MKRS=$(aws s3api list-object-versions --bucket "$UPB" --query '{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' --output json --region "$AWS_REGION" || echo '{"Objects":[]}')
                    if [ "$(echo "$MKRS" | python - <<'PY'
import sys, json; print(len(json.load(sys.stdin)["Objects"]))
PY
)" = "0" ]; then break; fi
                    aws s3api delete-objects --bucket "$UPB" --delete "$MKRS" --region "$AWS_REGION" || true
                  done
                  aws s3api delete-bucket --bucket "$UPB" --region "$AWS_REGION" || true
                fi
              fi
            fi
          done
          echo "‚ùå All DB instance classes failed"
          exit 1

      - name: Export CloudFormation outputs
        if: steps.deploy.outcome == 'success'
        id: cfn-outputs
        run: |
          set -euo pipefail
          STACK="$STACK_NAME"
          if [ "${{ inputs.stage || '' }}" != "" ] && [ "${{ inputs.stage }}" != "prod" ]; then
            STACK="subscriber-migration-portal-${{ inputs.stage }}"
          fi
          OUT=$(aws cloudformation describe-stacks --region "$AWS_REGION" --stack-name "$STACK" --query "Stacks[0].Outputs" --output json)
          echo "CFN_OUTPUTS=$OUT" >> $GITHUB_ENV
          python - <<'PY'
import os, json
outs=json.loads(os.environ['CFN_OUTPUTS'])
m={o['OutputKey']:o['OutputValue'] for o in outs}
with open(os.environ['GITHUB_ENV'],'a') as f:
  for k in ['ApiEndpoint','LegacyDbSecretArn','LegacyDbHost','SchemaInitFunctionName','UploadsBucketName']:
    f.write(f"{k}={m.get(k,'')}\n")
print('‚úÖ Outputs exported')
PY

      - name: Initialize MySQL schema (Lambda in VPC)
        if: steps.deploy.outcome == 'success'
        run: |
          set -euo pipefail
          echo "üîß Initializing schema via Lambda: $SCHEMA_INIT_FUNCTION_NAME in $AWS_REGION"
          aws lambda invoke \
            --region "$AWS_REGION" \
            --function-name "$SCHEMA_INIT_FUNCTION_NAME" \
            --payload '{}' \
            --log-type Tail \
            --query 'LogResult' \
            --output text \
            out.json | base64 -d || true
          echo
          cat out.json || true
          echo
          OK=$(cat out.json 2>/dev/null | python - <<'PY'
import sys, json; print(json.load(sys.stdin).get('success', False))
PY
 || echo False)
          if [ "$OK" != "True" ]; then
            echo "‚ùó Lambda schema init reported errors; running direct fallback"
          else
            echo "‚úÖ Lambda schema initialization completed"
            exit 0
          fi

      - name: Initialize MySQL schema (Direct runner fallback)
        if: steps.deploy.outcome == 'success'
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
          LEGACY_DB_SECRET_ARN: ${{ env.LegacyDbSecretArn }}
          LEGACY_DB_HOST: ${{ env.LegacyDbHost }}
        run: |
          set -euo pipefail
          python - <<'PY'
import os, json, boto3, pymysql, sys
region = os.environ.get('AWS_REGION','us-west-2')
secret_arn = os.environ.get('LEGACY_DB_SECRET_ARN','')
host = os.environ.get('LEGACY_DB_HOST','')
sql_file = 'database/rds_schema_update.sql'
if not secret_arn or not host:
    print('‚ùå Missing LegacyDbSecretArn/LegacyDbHost from outputs', file=sys.stderr)
    sys.exit(1)
if not os.path.exists(sql_file):
    print('‚ùå No schema file found', file=sys.stderr)
    sys.exit(1)
sm = boto3.client('secretsmanager', region_name=region)
sec = json.loads(sm.get_secret_value(SecretId=secret_arn)['SecretString'])
user = sec.get('username') or sec.get('user')
pwd  = sec.get('password') or sec.get('pass')
db   = sec.get('dbname') or sec.get('database') or ''
if not (user and pwd):
    print('‚ùå Secret missing username/password', file=sys.stderr)
    sys.exit(1)
conn = pymysql.connect(host=host, user=user, password=pwd, database=db, autocommit=True, connect_timeout=30, charset='utf8mb4')
executed = skipped = errors = 0
with conn.cursor() as cur, open(sql_file, 'r', encoding='utf-8') as f:
    sql = f.read()
    stmts = [s.strip() for s in sql.split(';') if s.strip() and not s.strip().startswith('--')]
    for stmt in stmts:
        try:
            cur.execute(stmt)
            executed += 1
            print(f'Executed: {stmt[:80]}...')
        except Exception as e:
            msg = str(e).lower()
            if 'already exists' in msg or 'duplicate' in msg or 'exists' in msg:
                skipped += 1
                print(f'Skip exists: {stmt[:80]}...')
            else:
                errors += 1
                print(f'Error executing {stmt[:80]}...: {e}', file=sys.stderr)
print(f'‚úÖ Schema init completed: executed={executed}, skipped={skipped}, errors={errors}')
if errors:
    sys.exit(1)
PY
